{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPIJ5SKXImmpJ2BwPUAJZQn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KingYuanSun/Individual-Projects-NLP/blob/master/Sentiment_Analysis_StockTrendPrediction_Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zIXSKk5Y_DI",
        "colab_type": "text"
      },
      "source": [
        "BERT with Sentiment analysis¶\n",
        "In this notebook I used a pretrained version of BERT avaliable as a huggingface transformed to classify the sentiment of news articles about Bitcoin and Tesla, and applied an LSTM to predict the stock returns\n",
        "\n",
        "Sources\n",
        "https://towardsdatascience.com/fine-tuning-bert-with-keras-and-tf-module-ed24ea91cff2\n",
        "\n",
        "Bert for dummies: https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03\n",
        "Bert for long texts: https://medium.com/@armandj.olivares/using-bert-for-classifying-documents-with-long-texts-5c3e7b04573d\n",
        "googles notebook: https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb\n",
        "strong.io: https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b\n",
        "strong.io notebook : https://github.com/strongio/keras-bert\n",
        "https://keras.io/layers/writing-your-own-keras-layers/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOWNlneVaHc8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe4cd550-d12b-4de6-bb66-f683e38745da"
      },
      "source": [
        "!pip install utils\n",
        "!pip install bert-for-tf2\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install nltk\n",
        "!pip install yfinance\n",
        "!pip install news-please\n",
        "!pip install google\n",
        "!pip install transformers"
      ],
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: utils in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (0.14.5)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.9.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.5)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.18.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.6/dist-packages (0.1.54)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.6/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.18.5)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from yfinance) (0.0.9)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.0.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.24->yfinance) (1.15.0)\n",
            "Requirement already satisfied: news-please in /usr/local/lib/python3.6/dist-packages (1.5.3)\n",
            "Requirement already satisfied: hurry.filesize>=0.9 in /usr/local/lib/python3.6/dist-packages (from news-please) (0.9)\n",
            "Requirement already satisfied: Scrapy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from news-please) (2.3.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.3.2 in /usr/local/lib/python3.6/dist-packages (from news-please) (4.6.3)\n",
            "Requirement already satisfied: awscli>=1.11.117 in /usr/local/lib/python3.6/dist-packages (from news-please) (1.18.118)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from news-please) (1.15.0)\n",
            "Requirement already satisfied: elasticsearch>=2.4 in /usr/local/lib/python3.6/dist-packages (from news-please) (7.8.1)\n",
            "Requirement already satisfied: readability-lxml>=0.6.2 in /usr/local/lib/python3.6/dist-packages (from news-please) (0.8.1)\n",
            "Requirement already satisfied: langdetect>=1.0.7 in /usr/local/lib/python3.6/dist-packages (from news-please) (1.0.8)\n",
            "Requirement already satisfied: plac>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from news-please) (1.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from news-please) (2.8.1)\n",
            "Requirement already satisfied: warcio>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from news-please) (1.7.4)\n",
            "Requirement already satisfied: PyMySQL>=0.7.9 in /usr/local/lib/python3.6/dist-packages (from news-please) (0.10.0)\n",
            "Requirement already satisfied: psycopg2-binary>=2.8.4 in /usr/local/lib/python3.6/dist-packages (from news-please) (2.8.5)\n",
            "Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.6/dist-packages (from news-please) (2.0.5)\n",
            "Requirement already satisfied: ago>=0.0.9 in /usr/local/lib/python3.6/dist-packages (from news-please) (0.0.93)\n",
            "Requirement already satisfied: dotmap>=1.2.17 in /usr/local/lib/python3.6/dist-packages (from news-please) (1.3.17)\n",
            "Requirement already satisfied: newspaper3k>=0.2.8 in /usr/local/lib/python3.6/dist-packages (from news-please) (0.2.8)\n",
            "Requirement already satisfied: lxml>=3.3.5 in /usr/local/lib/python3.6/dist-packages (from news-please) (4.2.6)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (from news-please) (0.0.1)\n",
            "Requirement already satisfied: hjson>=1.5.8 in /usr/local/lib/python3.6/dist-packages (from news-please) (3.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from hurry.filesize>=0.9->news-please) (49.2.0)\n",
            "Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (1.5.0)\n",
            "Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (1.1.0)\n",
            "Requirement already satisfied: zope.interface>=4.1.3 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (5.1.0)\n",
            "Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (0.1.0)\n",
            "Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (0.1.16)\n",
            "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (3.0)\n",
            "Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (1.6.0)\n",
            "Requirement already satisfied: pyOpenSSL>=16.2.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (19.1.0)\n",
            "Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (1.0.2)\n",
            "Requirement already satisfied: Twisted>=17.9.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (20.3.0)\n",
            "Requirement already satisfied: service-identity>=16.0.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (18.1.0)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (1.22.0)\n",
            "Requirement already satisfied: PyYAML<5.4,>=3.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.117->news-please) (3.13)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.117->news-please) (0.15.2)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.117->news-please) (0.3.3)\n",
            "Requirement already satisfied: rsa<=4.5.0,>=3.1.2 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.117->news-please) (4.5)\n",
            "Requirement already satisfied: colorama<0.4.4,>=0.2.5 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.117->news-please) (0.4.3)\n",
            "Requirement already satisfied: botocore==1.17.41 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.117->news-please) (1.17.41)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from elasticsearch>=2.4->news-please) (2020.6.20)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from elasticsearch>=2.4->news-please) (1.24.3)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.6/dist-packages (from readability-lxml>=0.6.2->news-please) (3.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (0.35.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (2.23.0)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (7.0.0)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (0.3)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (3.2.5)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (2.2.3)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (0.0.4)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (5.2.1)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.0->Scrapy>=1.1.0->news-please) (1.14.1)\n",
            "Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.6/dist-packages (from itemloaders>=1.0.1->Scrapy>=1.1.0->news-please) (0.10.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->Scrapy>=1.1.0->news-please) (15.1.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->Scrapy>=1.1.0->news-please) (19.3.0)\n",
            "Requirement already satisfied: PyHamcrest!=1.10.0,>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->Scrapy>=1.1.0->news-please) (2.0.2)\n",
            "Requirement already satisfied: Automat>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->Scrapy>=1.1.0->news-please) (20.2.0)\n",
            "Requirement already satisfied: incremental>=16.10.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->Scrapy>=1.1.0->news-please) (17.5.0)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->Scrapy>=1.1.0->news-please) (20.0.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->Scrapy>=1.1.0->news-please) (0.2.8)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->Scrapy>=1.1.0->news-please) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k>=0.2.8->news-please) (2.10)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k>=0.2.8->news-please) (1.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->Scrapy>=1.1.0->news-please) (2.20)\n",
            "Requirement already satisfied: google in /usr/local/lib/python3.6/dist-packages (2.0.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from google) (4.6.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oWOzCQAdm7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##utils.py\n",
        "\n",
        "import pandas as pd, numpy as np\n",
        "try:\n",
        "\timport bert\n",
        "except:\n",
        "\tprint(\"bert-for-tf2 not installed\")\n",
        "\n",
        "# transforms sentences to ids, masks and segment ids prepared to feed bert\n",
        "def convert_sentence_to_features(sentence, tokenizer, max_seq_len):\n",
        "    tokens = ['[CLS]']\n",
        "    tokens.extend(tokenizer.tokenize(sentence))\n",
        "    if len(tokens) > max_seq_len-1:\n",
        "        tokens = tokens[:max_seq_len-1]\n",
        "    tokens.append('[SEP]')\n",
        "    \n",
        "    segment_ids = [0] * len(tokens)\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    #Zero Mask till seq_length\n",
        "    zero_mask = [0] * (max_seq_len-len(tokens))\n",
        "    input_ids.extend(zero_mask)\n",
        "    input_mask.extend(zero_mask)\n",
        "    segment_ids.extend(zero_mask)\n",
        "    \n",
        "    return input_ids, input_mask, segment_ids\n",
        "\n",
        "def convert_sentences_to_features(sentences, tokenizer, max_seq_len=200):\n",
        "    all_input_ids = []\n",
        "    all_input_mask = []\n",
        "    all_segment_ids = []\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        input_ids, input_mask, segment_ids = convert_sentence_to_features(sentence, tokenizer, max_seq_len)\n",
        "        all_input_ids.append(input_ids)\n",
        "        all_input_mask.append(input_mask)\n",
        "        all_segment_ids.append(segment_ids)\n",
        "    \n",
        "    return all_input_ids, all_input_mask, all_segment_ids\n",
        "\n",
        "def generate_data_for_tokenizer(split_text,target_series):\n",
        "    labels_list = []\n",
        "    dates = []\n",
        "    for date, arrays in split_text.itertuples():\n",
        "        dates.extend([date]* len(arrays))\n",
        "    for date in dates:\n",
        "        labels_list.append(target_series.loc[date])\n",
        "    \n",
        "    split_text_flat = split_text.values.flatten()\n",
        "    sentence_list = [sentence for array in split_text_flat for sentence in array]\n",
        "    \n",
        "    labels = pd.DataFrame(labels_list, index = dates)\n",
        "    sentences  = pd.DataFrame(sentence_list, index = dates)\n",
        "    return sentences, labels\n",
        "\n",
        "# given an input text and a set of keywords, returns the top_n_terms which contain any of the keywords by frequency of appearance.\n",
        "def find_new_token_with_custom_keywords(array_of_text, custom_keywords, top_n_terms, extra_tokens):\n",
        "    \n",
        "    def contains_keyword(word,keywords):\n",
        "        for k in keywords:\n",
        "            if word.find(k) >= 0:\n",
        "                return True\n",
        "        return False\n",
        "    \n",
        "    def count_frequency(my_list): \n",
        "        freq = {} \n",
        "        for item in my_list: \n",
        "            if (item in freq): \n",
        "                freq[item] += 1\n",
        "            else: \n",
        "                freq[item] = 1\n",
        "        return freq\n",
        "    \n",
        "    raw_text = \"\".join(array_of_text).replace(\".com\",\"-com\").replace(\".\", \"\").replace(\",\", \"\").replace(\"\\n\", \" \").replace(\"-com\",\".com\")\n",
        "    raw_words = raw_text.split(\" \")\n",
        "    matches = []\n",
        "    for word in raw_words:\n",
        "        if contains_keyword(word.lower(),custom_keywords):\n",
        "            matches.append(word.lower())\n",
        "    \n",
        "    matches_count = count_frequency(matches)\n",
        "    #sorts the counts\n",
        "    #matches_dict = {k: v for k, v in sorted(matches_count.items(), key=lambda item: item[1], reverse = True)}\n",
        "    # selects top n words from the list\n",
        "    #new_tokens = list(matches_dict)[:top_n_terms]  + extra_tokens\n",
        "    import operator\n",
        "    sorted_x = sorted(matches_count.items(), key=operator.itemgetter(1), reverse = True)\n",
        "    new_tokens = [ tup[0] for tup in sorted_x[:top_n_terms]]  + extra_tokens\n",
        "    \n",
        "    print(\"New tokens to be added: \",new_tokens)\n",
        "    return new_tokens\n",
        "\n",
        "# creates bert tokenizer\n",
        "def create_tokenizer(vocab_file='vocab.txt', do_lower_case=True):\n",
        "    return bert.bert_tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "# appends extra tokens to the vocab of the tokenizer\n",
        "def add_new_tokens(new_vocab, tokenizer):\n",
        "    for i in range(len(new_vocab)):\n",
        "        new_key = new_vocab[i]\n",
        "        old_key = \"[unused{}]\".format(i)\n",
        "        value = tokenizer.vocab.pop(old_key)\n",
        "        tokenizer.vocab[new_key] = value\n",
        "    return tokenizer\n",
        "\n",
        "# transforms bet output in one continuous series removing the padding\n",
        "def bert_output_to_one_time_series_per_day(bert_inputs, bert_output, sentences):\n",
        "    n_sentences = sentences.groupby(sentences.index).count()\n",
        "    n_tokens = bert_inputs[\"input_mask\"][:].sum(axis = 1)\n",
        "    mask_out = [bert_output[1][counter,:length,:] for length, counter in zip(n_tokens,range(len(n_tokens)))]\n",
        "    \n",
        "    articles_per_day = []\n",
        "    acc = 0\n",
        "    for n in n_sentences.values:\n",
        "        n = n[0]\n",
        "        concat_articles = np.array(mask_out[acc:acc + n])\n",
        "        flattened = []\n",
        "        for sentence in concat_articles:\n",
        "            for token in sentence:\n",
        "                flattened.append(token)\n",
        "        flattened = np.array(flattened)\n",
        "        #flattened = np.array([token for token in sentence for sentence in concat_articles])\n",
        "        articles_per_day.append(flattened)\n",
        "        acc += n\n",
        "    return np.array(articles_per_day)\n",
        "\n",
        "# prepares_labels\n",
        "def label_transformer(prices, mode = \"returns\", shift = 5, index = None, standarized = False):\n",
        "    prices = pd.DataFrame(prices)\n",
        "    prices.columns = [\"today\"]\n",
        "    if index is not None:\n",
        "        prices = prices[index]\n",
        "    prices[\"tomorrow\"] = prices.shift(1)\n",
        "    prices[\"returns\"] = prices[\"today\"].pct_change()\n",
        "    prices[\"diff\"] = prices[\"today\"] - prices[\"tomorrow\"]\n",
        "    def standard(df):\n",
        "        return (df - df.mean())/df.std()\n",
        "    output = prices[mode].shift(shift).dropna()\n",
        "    return output if not standarized else standard(output)\n",
        "\n",
        "# computes and returns intersection among series\n",
        "def series_intersection(a,b):\n",
        "    a.index = pd.DatetimeIndex(a.index)\n",
        "    b.index = pd.DatetimeIndex(b.index)\n",
        "    intersection = pd.DatetimeIndex([value for value in a.index if value in b.index])\n",
        "    return a.loc[intersection], b.loc[intersection]\n",
        "\n",
        "\n",
        "\n",
        "def rolling_window_bert_2nd_dim(a, window):\n",
        "    shape = (a.shape[0] - window + 1, window, a.shape[1])\n",
        "    strides = (a.strides[0], a.strides[1]*a.shape[1], a.strides[1])\n",
        "    #print(shape, strides)\n",
        "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
        "\n",
        "\n",
        "def dummy():\n",
        "\treturn \"dah\""
      ],
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx1uhty9fHtR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "class Classifier_INCEPTION:\n",
        "\n",
        "    def __init__(self, output_directory, input_shape, nb_classes, verbose=False, build=True, batch_size=64,\n",
        "                 nb_filters=32, use_residual=True, use_bottleneck=True, depth=6, kernel_size=41, nb_epochs=1500):\n",
        "\n",
        "        self.output_directory = output_directory\n",
        "\n",
        "        self.nb_filters = nb_filters\n",
        "        self.use_residual = use_residual\n",
        "        self.use_bottleneck = use_bottleneck\n",
        "        self.depth = depth\n",
        "        self.kernel_size = kernel_size - 1\n",
        "        self.callbacks = None\n",
        "        self.batch_size = batch_size\n",
        "        self.bottleneck_size = 32\n",
        "        self.nb_epochs = nb_epochs\n",
        "\n",
        "        if build == True:\n",
        "            self.model = self.build_model(input_shape, nb_classes)\n",
        "            if (verbose == True):\n",
        "                self.model.summary()\n",
        "            self.verbose = verbose\n",
        "            self.model.save_weights(self.output_directory + 'model_init.hdf5')\n",
        "\n",
        "    def _inception_module(self, input_tensor, stride=1, activation='linear'):\n",
        "\n",
        "        if self.use_bottleneck and int(input_tensor.shape[-1]) > 1:\n",
        "            input_inception = keras.layers.Conv1D(filters=self.bottleneck_size, kernel_size=1,\n",
        "                                                  padding='same', activation=activation, use_bias=False)(input_tensor)\n",
        "        else:\n",
        "            input_inception = input_tensor\n",
        "\n",
        "        # kernel_size_s = [3, 5, 8, 11, 17]\n",
        "        kernel_size_s = [self.kernel_size // (2 ** i) for i in range(3)]\n",
        "\n",
        "        conv_list = []\n",
        "\n",
        "        for i in range(len(kernel_size_s)):\n",
        "            conv_list.append(keras.layers.Conv1D(filters=self.nb_filters, kernel_size=kernel_size_s[i],\n",
        "                                                 strides=stride, padding='same', activation=activation, use_bias=False)(\n",
        "                input_inception))\n",
        "\n",
        "        max_pool_1 = keras.layers.MaxPool1D(pool_size=3, strides=stride, padding='same')(input_tensor)\n",
        "\n",
        "        conv_6 = keras.layers.Conv1D(filters=self.nb_filters, kernel_size=1,\n",
        "                                     padding='same', activation=activation, use_bias=False)(max_pool_1)\n",
        "\n",
        "        conv_list.append(conv_6)\n",
        "\n",
        "        x = keras.layers.Concatenate(axis=2)(conv_list)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "        x = keras.layers.Activation(activation='relu')(x)\n",
        "        return x\n",
        "\n",
        "    def _shortcut_layer(self, input_tensor, out_tensor):\n",
        "        shortcut_y = keras.layers.Conv1D(filters=int(out_tensor.shape[-1]), kernel_size=1,\n",
        "                                         padding='same', use_bias=False)(input_tensor)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "        x = keras.layers.Add()([shortcut_y, out_tensor])\n",
        "        x = keras.layers.Activation('relu')(x)\n",
        "        return x\n",
        "\n",
        "    def build_layer_structure(self, input_tensor):\n",
        "        x = input_tensor\n",
        "        input_res = input_layer\n",
        "\n",
        "        for d in range(self.depth):\n",
        "\n",
        "            x = self._inception_module(x)\n",
        "\n",
        "            if self.use_residual and d % 3 == 2:\n",
        "                x = self._shortcut_layer(input_tensor, x)\n",
        "                input_res = x\n",
        "\n",
        "        gap_layer = keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "\n",
        "        return  gap_layer\n",
        "\n",
        "\n",
        "    def build_model(self, input_shape, nb_classes):\n",
        "        input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "        x = input_layer\n",
        "        input_res = input_layer\n",
        "\n",
        "        for d in range(self.depth):\n",
        "\n",
        "            x = self._inception_module(x)\n",
        "\n",
        "            if self.use_residual and d % 3 == 2:\n",
        "                x = self._shortcut_layer(input_res, x)\n",
        "                input_res = x\n",
        "\n",
        "        gap_layer = keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "        output_layer = keras.layers.Dense(nb_classes, activation='softmax')(gap_layer)\n",
        "\n",
        "        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50,\n",
        "                                                      min_lr=0.0001)\n",
        "\n",
        "        file_path = self.output_directory + 'best_model.hdf5'\n",
        "\n",
        "        model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss',\n",
        "                                                           save_best_only=True)\n",
        "\n",
        "        self.callbacks = [reduce_lr, model_checkpoint]\n",
        "\n",
        "        return model\n",
        "\n",
        "    def fit(self, x_train, y_train, x_val, y_val, y_true, plot_test_acc=False):\n",
        "\n",
        "        # x_val and y_val are only used to monitor the test loss and NOT for training\n",
        "\n",
        "        if self.batch_size is None:\n",
        "            mini_batch_size = int(min(x_train.shape[0] / 10, 16))\n",
        "        else:\n",
        "            mini_batch_size = self.batch_size\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        if plot_test_acc:\n",
        "\n",
        "            hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=self.nb_epochs,\n",
        "                                  verbose=self.verbose, validation_data=(x_val, y_val), callbacks=self.callbacks)\n",
        "        else:\n",
        "\n",
        "            hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=self.nb_epochs,\n",
        "                                  verbose=self.verbose, callbacks=self.callbacks)\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        self.model.save(self.output_directory + 'last_model.hdf5')\n",
        "\n",
        "        y_pred = self.predict(x_val, y_true, x_train, y_train, y_val)\n",
        "\n",
        "        # save predictions\n",
        "        np.save(self.output_directory + 'y_pred.npy', y_pred)\n",
        "\n",
        "        # convert the predicted from binary to integer\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "        keras.backend.clear_session()\n",
        "\n",
        "        return hist\n",
        "\n",
        "    def predict(self, x_test, y_true, x_train, y_train, y_test):\n",
        "        start_time = time.time()\n",
        "        model_path = self.output_directory + 'best_model.hdf5'\n",
        "        model = keras.models.load_model(model_path)\n",
        "        y_pred = model.predict(x_test, batch_size=self.batch_size)\n",
        "        return y_pred"
      ],
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOFMsxaTZK9V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fc18c0d0-4dcf-4326-84bd-95f4648cce07"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "# !ls \"/content/drive/My Drive\""
      ],
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxg9lgNhZBQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd, numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from utils import *\n",
        "\n",
        "import importlib\n",
        "import utils\n",
        "importlib.reload(utils)\n",
        "from utils import *\n",
        "\n",
        "import yfinance as yf\n",
        "\n",
        "start = \"2019-01-01\"\n",
        "end   = \"2020-03-20\"\n",
        "\n",
        "bitcoin = False\n",
        "if not bitcoin:   \n",
        "    stocks = [\"TSLA\"]\n",
        "    keywords = {\"TSLA\": [\"Tesla\", \"Elon Musk\"]}\n",
        "else:    \n",
        "    stocks = [\"BTC-USD\"]\n",
        "    keywords = {\"BTC-USD\" : [\"Bitcoin\", \"Cryptocurrency\"] }\n",
        "\n",
        "df_financial = yf.download(stocks, \n",
        "                     #period = \"1Y\",\n",
        "                      start= start, \n",
        "                      end= end, \n",
        "                      progress=False)\n",
        "prices = df_financial[\"Close\"]\n",
        "prices = pd.DataFrame(data = prices, index = pd.date_range(start,end)).fillna(method = \"bfill\")\n",
        "\n",
        "df_bitcoin = pd.read_csv(\"/content/drive/My Drive/ColabData/articles_Bitcoin-Cryptocurrency_start=2019-01-01_end=2020-12-31.csv\", index_col = 0)\n",
        "df_tesla = pd.read_csv(\"/content/drive/My Drive/ColabData/articles_Tesla-Elon_Musk_start=2019-01-01_end=2020-12-31.csv\", index_col = 0)\n",
        "df = df_bitcoin if bitcoin else df_tesla\n",
        "\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, AutoModelForSequenceClassification, TFBertForSequenceClassification \n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")"
      ],
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caiPUUFkcEwe",
        "colab_type": "text"
      },
      "source": [
        "Split sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkbmlgJrcGmZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "8b221b94-4242-4260-9a6b-8b0e7678a47f"
      },
      "source": [
        "main_text = False\n",
        "if main_text:\n",
        "    # main_text\n",
        "    text = df.maintext #pd.DataFrame(df.maintext.values, index = df[\"date_google\"]).dropna()\n",
        "    text.index = df[\"date_google\"]\n",
        "else:    \n",
        "    # titles and descriptions\n",
        "    titles = df.title\n",
        "    titles.index = df[\"date_google\"]\n",
        "    descriptions = df.description\n",
        "    descriptions.index = df[\"date_google\"]\n",
        "    text = pd.concat([titles,descriptions]).sort_index().dropna()\n",
        "    \n",
        "print(text.values[0])\n",
        "\n",
        "import nltk.data\n",
        "nltk.download('punkt')\n",
        "split_sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "split_sentences = text.apply(split_sentence_tokenizer.tokenize)\n",
        "split_sentences = split_sentences.groupby(split_sentences.index).sum()\n",
        "split_sentences = pd.DataFrame(split_sentences)\n",
        "\n",
        "df_ret = label_transformer(prices.copy(), shift = 1)\n",
        "\n",
        "split_sentences, df_ret = series_intersection(split_sentences, df_ret)\n",
        "raw_sentences, raw_labels = generate_data_for_tokenizer(split_sentences,df_ret)\n",
        "\n",
        "lengths = raw_sentences.apply(lambda x: len(x[0].split()), axis = 1)\n",
        "sentences = raw_sentences[(lengths > 10) & (lengths < 120)] #filter short and long sentences\n",
        "labels = raw_labels[(lengths > 10) & (lengths < 120)]\n",
        "\n",
        "keywords_bitcoin = [\"crypto\", \"BTC\", \"bitcoin\", \"blockchain\"]\n",
        "keywords_tesla =   [\"tesla\", \"Elon\", \"Musk\", \"TSLA\", \"Tesla\"]\n",
        "keys = keywords_bitcoin if bitcoin else keywords_tesla\n",
        "raw_text = \"\".join(text.values).replace(\".com\",\"-com\").replace(\".\", \"\").replace(\",\", \"\").replace(\"\\n\", \" \").replace(\"-com\",\".com\")\n",
        "new_tokens = find_new_token_with_custom_keywords(raw_text, keys , 6, [])\n",
        "# new_tokens = new_tokens.append('tesla\\'s')\n",
        "\n",
        "print (new_tokens)\n",
        "print (sentences[0])\n",
        "print (sentences[0][19])\n",
        "\n",
        "print (sentences.dtypes)\n",
        "\n",
        "res = [sub.replace('Tesla\\'s', 'tesla').replace('Tesla’s', 'tesla').replace('tesla\\'s', 'tesla').replace('tesla’s', 'tesla').replace('Teslas', 'tesla').replace('teslaelon', 'tesla').replace('musktesla', 'tesla') for sub in sentences[0]] \n",
        "\n",
        "# tesla’s Teslas\n",
        "\n",
        "sentences = pd.Series(res) \n",
        "print (sentences[19])\n",
        "print (sentences[103])\n",
        "# sentences = []\n",
        "# sentences.append(res)\n",
        "# print (sentences[0][19])\n",
        "# print (res[19])"
      ],
      "execution_count": 294,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Conor McGregor compared himself to Elon Musk in fiery exchange with Floyd Mayweather's manager\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "New tokens to be added:  ['tesla', \"tesla's\", 'tesla’s', 'teslas', 'teslaelon', 'musktesla']\n",
            "['tesla', \"tesla's\", 'tesla’s', 'teslas', 'teslaelon', 'musktesla']\n",
            "2019-01-03    Tesla CEO Elon Musk doubled down on criticism ...\n",
            "2019-01-03    It’s a counterintuitive result to mention toda...\n",
            "2019-01-03    But over a century of research on work environ...\n",
            "2019-01-03    Elon Musk tweeted that Singapore has been unwe...\n",
            "2019-01-03    From a Japanese mathematician to Elon Musk, he...\n",
            "                                    ...                        \n",
            "2020-03-19    Tesla Cybertruck Gigafactory: Oklahoma pulls o...\n",
            "2020-03-19    NYC mayor asks Elon Musk to manufacture ventil...\n",
            "2020-03-19    Elon Musk downplays coronavirus as Tesla facto...\n",
            "2020-03-19    Elon Musk took to Twitter yesterday to claim h...\n",
            "2020-03-19    The need is clear, let's hope he comes through...\n",
            "Name: 0, Length: 3186, dtype: object\n",
            "Elon Musk's 'Blastar' would be a perfect addition to Tesla's Easter Eggs\n",
            "0    object\n",
            "dtype: object\n",
            "Elon Musk's 'Blastar' would be a perfect addition to tesla Easter Eggs\n",
            "tesla customer referral program, in which Tesla owners can give their friends a referral code to get six months of free charging via tesla Supercharger network, will be ending on March 1st, Elon Musk said in a tweet.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a77gW9KSd2od",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f70e7b7c-8594-4976-d91c-5f3dccc7230e"
      },
      "source": [
        "#model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\", from_pt=True)\n",
        "tokenizer.add_tokens(new_tokens)"
      ],
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFBertForSequenceClassification were initialized from the TF 2.0 model.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 295
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Os2JgdSeVsh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "36fea2c8-aae8-4444-ab57-04aa712fe754"
      },
      "source": [
        "example = \"Bitcoin futures are trading below the cryptocurrency's spot price\"\n",
        "print(tokenizer.tokenize(example))\n",
        "example_ids = tokenizer.encode(example)\n",
        "print(example_ids)\n",
        "output = model.predict([example_ids]) \n",
        "print(\"    1 star     2 stars     3 stars     4 stars     5 stars\")\n",
        "print(output[0])"
      ],
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['bit', '##co', '##in', 'futures', 'are', 'trading', 'below', 'the', 'cry', '##pt', '##oc', '##urre', '##ncy', \"'\", 's', 'spot', 'price']\n",
            "[101, 16464, 10805, 10262, 42272, 10320, 34948, 16934, 10103, 29917, 15903, 20731, 46642, 19771, 112, 161, 24311, 16993, 102]\n",
            "    1 star     2 stars     3 stars     4 stars     5 stars\n",
            "[[ 1.1589032   0.6475529   0.23367804 -0.6871393  -1.1347752 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0-vmro8efP2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "39de0332-bb41-433f-8752-dd7ce4e11c85"
      },
      "source": [
        "example = \"I am so disappointed with this product\"\n",
        "print(tokenizer.tokenize(example))\n",
        "example_ids = tokenizer.encode(example)\n",
        "print(example_ids)\n",
        "output = model.predict([example_ids])\n",
        "print(\"    1 star     2 stars    3 stars    4 stars    5 stars\")\n",
        "print(output[0])"
      ],
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'am', 'so', 'disa', '##ppo', '##inted', 'with', 'this', 'product']\n",
            "[101, 151, 10345, 10297, 31021, 54894, 83912, 10171, 10372, 20058, 102]\n",
            "WARNING:tensorflow:5 out of the last 6377 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0b54abeb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "    1 star     2 stars    3 stars    4 stars    5 stars\n",
            "[[ 3.3838353  2.8609188  0.6469802 -2.6131783 -3.5148158]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTnZvF8p9fHD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "57595549-67a8-40b4-aba4-12a98dbb1bf0"
      },
      "source": [
        "example = \"Elon Musk's 'Blastar' would be a perfect addition to tesla Easter Eggs\"\n",
        "\n",
        "\n",
        "print(tokenizer.tokenize(example))\n",
        "example_ids = tokenizer.encode(example)\n",
        "print(example_ids)\n",
        "output = model.predict([example_ids])\n",
        "print(\"    1 star     2 stars    3 stars    4 stars    5 stars\")\n",
        "print(output[0])"
      ],
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['elo', '##n', 'mus', '##k', \"'\", 's', \"'\", 'blast', '##ar', \"'\", 'would', 'be', 'a', 'perfect', 'addition', 'to', 'tesla', 'easter', 'eggs']\n",
            "[101, 21834, 10115, 23139, 10167, 112, 161, 112, 47732, 10370, 112, 11008, 10346, 143, 23021, 15000, 10114, 51571, 58776, 48540, 102]\n",
            "WARNING:tensorflow:6 out of the last 6378 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0b54abeb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "    1 star     2 stars    3 stars    4 stars    5 stars\n",
            "[[-1.48236    -0.895975    0.41832617  1.1279131   0.57270813]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IksNea52Qwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = sentences.apply(lambda x : tokenizer.tokenize(x))"
      ],
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBXhyvj6elaM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "40a8f885-82c5-486d-ab5a-d4227f2d0cbe"
      },
      "source": [
        "encoded_sentences = sentences.apply(lambda x : tokenizer.encode(x))\n",
        "print (sentences)\n",
        "print (encoded_sentences)\n",
        "print (encoded_sentences[0])\n",
        "print (encoded_sentences[1])\n",
        "encoded_sentences.shape"
      ],
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       [tesla, ceo, elo, ##n, mus, ##k, double, ##d, ...\n",
            "1       [it, [UNK], s, a, counter, ##int, ##uit, ##ive...\n",
            "2       [but, over, a, century, of, research, on, work...\n",
            "3       [elo, ##n, mus, ##k, twee, ##ted, that, singap...\n",
            "4       [from, a, japanese, math, ##ema, ##tician, to,...\n",
            "                              ...                        \n",
            "3181    [tesla, cyber, ##tru, ##ck, gi, ##ga, ##fa, ##...\n",
            "3182    [nyc, mayor, asks, elo, ##n, mus, ##k, to, man...\n",
            "3183    [elo, ##n, mus, ##k, down, ##play, ##s, corona...\n",
            "3184    [elo, ##n, mus, ##k, took, to, twitter, yester...\n",
            "3185    [the, need, is, clear, ,, let, ', s, hope, he,...\n",
            "Length: 3186, dtype: object\n",
            "0       [101, 51571, 23693, 21834, 10115, 23139, 10167...\n",
            "1       [101, 10197, 100, 161, 143, 32964, 16790, 1754...\n",
            "2       [101, 10502, 10323, 143, 11516, 10108, 11865, ...\n",
            "3       [101, 21834, 10115, 23139, 10167, 12915, 11894...\n",
            "4       [101, 10195, 143, 14201, 33508, 17291, 95605, ...\n",
            "                              ...                        \n",
            "3181    [101, 51571, 69742, 32831, 11732, 21464, 10547...\n",
            "3182    [101, 95561, 12263, 51329, 21834, 10115, 23139...\n",
            "3183    [101, 21834, 10115, 23139, 10167, 12090, 64653...\n",
            "3184    [101, 21834, 10115, 23139, 10167, 12384, 10114...\n",
            "3185    [101, 10103, 15415, 10127, 20674, 117, 12421, ...\n",
            "Length: 3186, dtype: object\n",
            "[101, 51571, 23693, 21834, 10115, 23139, 10167, 13965, 10163, 12090, 10125, 36103, 10108, 19649, 10125, 63585, 117, 22811, 10203, 10103, 11409, 10575, 118, 10652, 10438, 10662, 107, 10119, 24414, 21567, 10111, 107, 10114, 10235, 12485, 53030, 119, 102]\n",
            "[101, 10197, 100, 161, 143, 32964, 16790, 17540, 12899, 14712, 10114, 29583, 13980, 117, 19772, 10104, 143, 11828, 10203, 92306, 10107, 143, 35563, 118, 10103, 118, 29061, 118, 16256, 117, 11497, 118, 11573, 118, 10855, 118, 26987, 23314, 12705, 119, 102]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3186,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 300
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy6dMkyPiDLZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "deea46a9-2d61-40e3-f35d-67755100245b"
      },
      "source": [
        "# if we have \"tesla's\", we failed\n",
        "\n",
        "# print (encoded_sentences[0])\n",
        "# print (model.predict([encoded_sentences[0]]))\n",
        "# print (model.predict([encoded_sentences[0]])[0])\n",
        "\n",
        "# print (sentences[0][3])\n",
        "# print (sentences[0][4])\n",
        "# print (sentences[0][5])\n",
        "# print (sentences[0][6])\n",
        "# print (sentences[0][7])\n",
        "# print (sentences[0][9])\n",
        "# print (sentences[0][10])\n",
        "# print (sentences[0][16])\n",
        "# print (sentences[0][17])\n",
        "# print (sentences[0][18])\n",
        "# print (sentences[0][19])\n",
        "# print (sentences[0][20])\n",
        "\n",
        "# print (sentences[0][18])\n",
        "# print (encoded_sentences[18])\n",
        "# print (model.predict([encoded_sentences[18]]))\n",
        "# print (model.predict([encoded_sentences[18]])[0])\n",
        "\n",
        "print (sentences[10])\n",
        "print (encoded_sentences[10])\n",
        "print (model.predict(encoded_sentences[10]))\n",
        "print (model.predict([encoded_sentences[10]])[0])\n",
        "\n",
        "print (sentences[19])\n",
        "print (encoded_sentences[19])\n",
        "print (model.predict([encoded_sentences[19]]))\n",
        "print (model.predict([encoded_sentences[19]])[0])\n",
        "\n",
        "# for i in range(1, len(encoded_sentences)): \n",
        "\n",
        "#     print (i)\n",
        "#     print (sentences[i])\n",
        "#     print(encoded_sentences[i]) \n",
        "#     output = model.predict([encoded_sentences[i]])[0]\n",
        "#     print (output)\n",
        "\n",
        "\n"
      ],
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['investors', 'appeared', 'to', 'be', 'focusing', 'more', 'on', 'potential', 'road', '##bl', '##ock', '##s', ',', 'including', 'a', 'flood', 'of', 'new', 'competition', 'and', 'the', 'phase', '-', 'out', 'of', 'the', 'federal', 'tax', 'credits', 'that', 'offs', '##et', 'the', 'cost', 'of', 'tesla', 'battery', '-', 'electric', 'vehicles', '.']\n",
            "[101, 67446, 14889, 10114, 10346, 67493, 10772, 10125, 21570, 11925, 31809, 21906, 10107, 117, 11371, 143, 40241, 10108, 10246, 14262, 10110, 10103, 17324, 118, 10871, 10108, 10103, 12501, 22389, 33896, 10203, 47117, 10337, 10103, 18153, 10108, 51571, 34794, 118, 15988, 25327, 119, 102]\n",
            "WARNING:tensorflow:7 out of the last 6379 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0b54abeb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "(array([[ 2.10614756e-01,  2.56466150e-01,  1.39126316e-01,\n",
            "        -2.67221451e-01, -1.65504664e-01],\n",
            "       [-4.38403189e-01,  5.69914579e-01,  7.56413877e-01,\n",
            "         2.80110668e-02, -6.12575054e-01],\n",
            "       [ 1.57692879e-01,  2.46165216e-01,  1.76105902e-01,\n",
            "        -2.54177243e-01, -1.54445529e-01],\n",
            "       [ 8.45648944e-02,  2.18766451e-01,  1.93011418e-01,\n",
            "        -2.12201223e-01, -1.44217163e-01],\n",
            "       [ 1.29186986e-02,  1.80537850e-01,  2.26959333e-01,\n",
            "        -1.69549808e-01, -1.11226961e-01],\n",
            "       [-8.91145468e-01, -1.72820419e-01,  6.02216661e-01,\n",
            "         4.37138468e-01,  1.00002326e-01],\n",
            "       [ 2.25782290e-01,  2.69848466e-01,  1.32788315e-01,\n",
            "        -2.79277653e-01, -1.77465022e-01],\n",
            "       [ 1.79127634e-01,  2.41989598e-01,  1.54304281e-01,\n",
            "        -2.54679471e-01, -1.46387428e-01],\n",
            "       [-9.81200952e-03,  1.78980425e-01,  2.71497607e-01,\n",
            "        -1.50241524e-01, -1.28665134e-01],\n",
            "       [ 1.26667172e-01,  2.30678022e-01,  1.82913259e-01,\n",
            "        -2.23044619e-01, -1.49072260e-01],\n",
            "       [ 2.31278375e-01,  2.61766970e-01,  1.31136537e-01,\n",
            "        -2.76586473e-01, -1.70786619e-01],\n",
            "       [ 2.52902955e-01,  2.75030583e-01,  1.23805419e-01,\n",
            "        -2.88558275e-01, -1.82458580e-01],\n",
            "       [-6.78583622e-01, -1.77490324e-01,  5.31619966e-01,\n",
            "         3.41869622e-01, -3.22050750e-02],\n",
            "       [ 2.12556213e-01,  2.63698339e-01,  1.41411498e-01,\n",
            "        -2.71481454e-01, -1.68973058e-01],\n",
            "       [ 1.94894895e-01,  2.57199913e-01,  1.48490950e-01,\n",
            "        -2.64713854e-01, -1.67217076e-01],\n",
            "       [ 2.03230083e-01,  2.57784098e-01,  1.40857592e-01,\n",
            "        -2.71474928e-01, -1.56926766e-01],\n",
            "       [ 1.17809504e-01,  2.55242437e-01,  1.94721207e-01,\n",
            "        -2.32384995e-01, -1.71897531e-01],\n",
            "       [ 2.14793697e-01,  2.64461756e-01,  1.35594741e-01,\n",
            "        -2.75538951e-01, -1.70973867e-01],\n",
            "       [ 1.74926415e-01,  2.58555412e-01,  1.58152193e-01,\n",
            "        -2.58889616e-01, -1.63912460e-01],\n",
            "       [-5.36126912e-01, -2.94747800e-02,  3.73748362e-01,\n",
            "         1.07825130e-01,  2.07239911e-02],\n",
            "       [ 2.98720840e-02,  1.87490165e-01,  2.01014638e-01,\n",
            "        -1.85672089e-01, -9.59026366e-02],\n",
            "       [ 2.13059455e-01,  2.59260416e-01,  1.40896425e-01,\n",
            "        -2.72425741e-01, -1.67875364e-01],\n",
            "       [-4.67417121e-01,  4.07968834e-02,  5.07152259e-01,\n",
            "         9.46606994e-02, -1.38397902e-01],\n",
            "       [ 2.27628097e-01,  2.65107095e-01,  1.32422641e-01,\n",
            "        -2.76434451e-01, -1.72262743e-01],\n",
            "       [ 1.99365422e-01,  2.56296754e-01,  1.41474292e-01,\n",
            "        -2.63058662e-01, -1.62857324e-01],\n",
            "       [ 2.14793697e-01,  2.64461756e-01,  1.35594741e-01,\n",
            "        -2.75538951e-01, -1.70973867e-01],\n",
            "       [ 2.13059455e-01,  2.59260416e-01,  1.40896425e-01,\n",
            "        -2.72425741e-01, -1.67875364e-01],\n",
            "       [ 2.06675440e-01,  2.62889504e-01,  1.39268741e-01,\n",
            "        -2.59814978e-01, -1.71965867e-01],\n",
            "       [-4.23671037e-01, -4.23058867e-02,  3.79003406e-01,\n",
            "         7.92797953e-02,  6.04599118e-02],\n",
            "       [-7.83001482e-02,  1.68798894e-01,  2.61027694e-01,\n",
            "        -1.17394492e-01, -1.13812238e-01],\n",
            "       [-5.81604898e-01, -8.72868001e-02,  5.12219727e-01,\n",
            "         2.28117332e-01, -6.30621836e-02],\n",
            "       [-1.95325673e-01,  1.17701814e-01,  6.07129991e-01,\n",
            "        -1.04544073e-01, -4.75132883e-01],\n",
            "       [ 2.19776988e-01,  2.65178710e-01,  1.40754238e-01,\n",
            "        -2.78827220e-01, -1.69953644e-01],\n",
            "       [ 2.13059455e-01,  2.59260416e-01,  1.40896425e-01,\n",
            "        -2.72425741e-01, -1.67875364e-01],\n",
            "       [-5.52082598e-01, -1.00836813e-01,  4.45466757e-01,\n",
            "         2.29932919e-01, -6.75447809e-04],\n",
            "       [ 2.14793697e-01,  2.64461756e-01,  1.35594741e-01,\n",
            "        -2.75538951e-01, -1.70973867e-01],\n",
            "       [-4.29500669e-01, -1.38933852e-01,  4.35479492e-01,\n",
            "         2.10100904e-01,  9.70722642e-03],\n",
            "       [ 2.10718989e-01,  2.66877085e-01,  1.38648048e-01,\n",
            "        -2.79471427e-01, -1.68097064e-01],\n",
            "       [ 2.27628097e-01,  2.65107095e-01,  1.32422641e-01,\n",
            "        -2.76434451e-01, -1.72262743e-01],\n",
            "       [-4.94985849e-01, -1.36421010e-01,  4.31112617e-01,\n",
            "         2.50888944e-01,  5.38539700e-02],\n",
            "       [-6.52779359e-03,  1.66868418e-01,  2.37435922e-01,\n",
            "        -1.64166465e-01, -9.02409777e-02],\n",
            "       [ 1.69861376e-01,  2.39053950e-01,  1.61050335e-01,\n",
            "        -2.43647560e-01, -1.54808730e-01],\n",
            "       [-1.01045378e-01,  8.34390447e-02,  2.13062808e-01,\n",
            "        -9.35763642e-02, -1.83447227e-02]], dtype=float32),)\n",
            "[[ 0.5315337   1.0476683   0.57167995 -0.39557034 -1.5171793 ]]\n",
            "['elo', '##n', 'mus', '##k', \"'\", 's', \"'\", 'blast', '##ar', \"'\", 'would', 'be', 'a', 'perfect', 'addition', 'to', 'tesla', 'easter', 'eggs']\n",
            "[101, 21834, 10115, 23139, 10167, 112, 161, 112, 47732, 10370, 112, 11008, 10346, 143, 23021, 15000, 10114, 51571, 58776, 48540, 102]\n",
            "(array([[-1.48236   , -0.895975  ,  0.41832617,  1.1279131 ,  0.57270813]],\n",
            "      dtype=float32),)\n",
            "[[-1.48236    -0.895975    0.41832617  1.1279131   0.57270813]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL49QLltqP-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = encoded_sentences.apply(lambda x : model.predict([x])[0])\n"
      ],
      "execution_count": 302,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKUHsrxCSVAp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "92bc9ebc-6306-4f63-c78b-a14d0d70f002"
      },
      "source": [
        "predictions.shape\n"
      ],
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3186,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 303
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VxnQbSXSaee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_predictions = predictions.groupby(predictions.index).apply(np.mean)\n",
        "avg_predictions_array = np.array([a for a in avg_predictions.values])\n",
        "shape = avg_predictions_array.shape\n",
        "avg_predictions = pd.DataFrame(avg_predictions_array.reshape((shape[0], shape[2])), index = avg_predictions.index)\n",
        "pd.to_pickle(avg_predictions, \"sentiment_predictions_tesla_with_tokens\")"
      ],
      "execution_count": 304,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnNMGmJ_Sd2m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7f4102d-4f7f-4dd2-efc6-55aa0f2a4ee0"
      },
      "source": [
        "avg_predictions.shape"
      ],
      "execution_count": 305,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3186, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 305
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBK2XUNXf0Ly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import LSTM, Dense, Input, Flatten\n",
        "from tensorflow.keras.layers import TimeDistributed, Dropout, BatchNormalization, MaxPooling1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "def diff2p(d, start):\n",
        "    prices = []\n",
        "    cum_diff = start\n",
        "    for diff in d:\n",
        "        prices.append(diff + cum_diff)\n",
        "        cum_diff += diff\n",
        "    return prices\n",
        "\n",
        "def r2p(d, start = 100):\n",
        "    return start * (1 + d).cumprod()\n",
        "\n",
        "def rolling_window(a, window, step_size):\n",
        "    shape = a.shape[:-1] + (a.shape[-1] - window + 1 - step_size + 1, window)\n",
        "    strides = a.strides + (a.strides[-1] * step_size,)\n",
        "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
        "\n",
        "def rolling_window_bert_2nd_dim(a, window):\n",
        "    shape = (a.shape[0] - window + 1, window, a.shape[1])\n",
        "    strides = (a.strides[0], a.strides[1]*a.shape[1], a.strides[1])\n",
        "    #print(shape, strides)\n",
        "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
        "\n",
        "def lstm_model(n_steps):\n",
        "    n_features = 5\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(100, input_shape = (n_steps, n_features), return_sequences = True))\n",
        "    model.add(TimeDistributed(Dense(20, activation='elu')))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='elu'))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "def dense_model():\n",
        "    n_features = 5\n",
        "    model = Sequential()\n",
        "    #model.add(CuDNNLSTM(200, input_shape=(None, n_features), return_sequences= True))\n",
        "    model.add(Input((n_features)))\n",
        "    model.add(Dense(200))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(50, activation='tanh'))\n",
        "    model.add(Dense(1, activation='tanh'))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 306,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS5_nJZrT8jo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def destandarize(data, mean, std):\n",
        "    return data*std + mean\n",
        "\n",
        "def standarize(data):\n",
        "    return (data - data.mean())/data.std(), data.mean(), data.std()\n",
        "\n",
        "def lstm_train_and_predict(X_train, X_test, y_train, y_test, window, standarize_output = False, mode = \"returns\", start = 100, epochs = 10):\n",
        "    \n",
        "    y_test_orig = y_test.values\n",
        "    y_train_orig = y_train.values\n",
        "    \n",
        "    if standarize_output:\n",
        "        y_test, y_test_mean, y_test_std = standarize(y_test)\n",
        "        y_train, y_train_mean, y_train_std = standarize(y_train)\n",
        "    \n",
        "    X_train_roll = rolling_window_bert_2nd_dim(X_train.values, window)\n",
        "    X_test_roll = rolling_window_bert_2nd_dim(X_test.values, window)\n",
        "    y_train_roll = y_train[window - 1:].values\n",
        "    y_test_roll = y_test[window - 1:].values\n",
        "    \n",
        "    lstm = lstm_model(window)\n",
        "    history = lstm.fit(X_train_roll,y_train_roll, validation_data=(X_test_roll, y_test_roll), epochs = epochs)\n",
        "    pred_tr = lstm.predict(X_train_roll)\n",
        "    pred_te = lstm.predict(X_test_roll)\n",
        "    \n",
        "    if standarize_output:\n",
        "        pred_tr = destandarize(pred_tr,y_train_mean, y_train_std)\n",
        "        pred_te = destandarize(pred_te,y_test_mean, y_test_std)\n",
        "        y_train_roll = destandarize(y_train_roll,y_train_mean, y_train_std)\n",
        "        y_test_roll = destandarize(y_test_roll,y_test_mean, y_test_std)\n",
        "    \n",
        "    if mode == \"returns\":\n",
        "        pred_prices_tr = r2p(pred_tr, start)\n",
        "        pred_prices_te = r2p(pred_te, start)\n",
        "        real_prices_tr = r2p(y_train_orig, start)\n",
        "        real_prices_te = r2p(y_test_orig, start)\n",
        "        \n",
        "    if mode == \"diff\":\n",
        "        pred_prices_tr = diff2p(pred_tr, start)\n",
        "        pred_prices_te = diff2p(pred_te, start)\n",
        "        real_prices_tr = diff2p(y_train_orig, start)\n",
        "        real_prices_te = diff2p(y_test_orig, start)\n",
        "    \n",
        "    print(\"TEST DATA\")\n",
        "    plt.plot(real_prices_te)\n",
        "    plt.plot(pred_prices_te)\n",
        "    plt.legend([\"y_real\", \"y_pred\"])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    print(\"TRAIN DATA\")\n",
        "    plt.plot(real_prices_tr)\n",
        "    plt.plot(pred_prices_tr)\n",
        "    plt.legend([\"y_real\", \"y_pred\"])\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(history.history[\"loss\"])\n",
        "    plt.plot(history.history[\"val_loss\"])\n",
        "    plt.legend([\"loss\", \"val_loss\"])\n",
        "    plt.show()\n",
        "    return lstm, pred_tr, pred_te"
      ],
      "execution_count": 307,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViZwpXzxgSkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mode = \"returns\"\n",
        "shift = 1\n",
        "df_prices = label_transformer(prices.copy(), shift = shift, mode = mode)\n",
        "avg_predictions, df_prices = series_intersection(avg_predictions, df_prices)\n",
        "\n",
        "threshold = \"2019-11-01\"\n",
        "X_train = avg_predictions.loc[:threshold]\n",
        "X_test = avg_predictions.loc[threshold:]\n",
        "y_train = df_prices.loc[:threshold]\n",
        "y_test = df_prices.loc[threshold:]\n",
        "\n",
        "window = 7\n",
        "ep = 15\n",
        "\n",
        "# lstm, pred_tr, pred_te = lstm_train_and_predict(X_train, X_test, y_train, y_test, window, \n",
        "#                                                 standarize_output = False, mode = mode,\n",
        "#                                                 start = 100, epochs = ep)\n"
      ],
      "execution_count": 308,
      "outputs": []
    }
  ]
}