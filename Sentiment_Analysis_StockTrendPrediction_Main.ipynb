{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOwwctxSn+N5PDSM8I/GETp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KingYuanSun/Individual-Projects-NLP/blob/master/Sentiment_Analysis_StockTrendPrediction_Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zIXSKk5Y_DI",
        "colab_type": "text"
      },
      "source": [
        "BERT with Sentiment analysis¶\n",
        "In this notebook I used a pretrained version of BERT avaliable as a huggingface transformed to classify the sentiment of news articles about Bitcoin and Tesla, and applied an LSTM to predict the stock returns\n",
        "\n",
        "Sources\n",
        "https://towardsdatascience.com/fine-tuning-bert-with-keras-and-tf-module-ed24ea91cff2\n",
        "\n",
        "Bert for dummies: https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03\n",
        "Bert for long texts: https://medium.com/@armandj.olivares/using-bert-for-classifying-documents-with-long-texts-5c3e7b04573d\n",
        "googles notebook: https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb\n",
        "strong.io: https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b\n",
        "strong.io notebook : https://github.com/strongio/keras-bert\n",
        "https://keras.io/layers/writing-your-own-keras-layers/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOWNlneVaHc8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1104dfc8-08f7-4c97-f268-0aa33f5799a6"
      },
      "source": [
        "!pip install utils\n",
        "!pip install bert-for-tf2\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install nltk\n",
        "!pip install yfinance\n",
        "!pip install news-please\n",
        "!pip install google\n",
        "!pip install transformers"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: utils in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (0.14.5)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.9.7)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.5)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.6/dist-packages (0.1.54)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.18.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.6/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.0.5)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from yfinance) (0.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.24->yfinance) (1.15.0)\n",
            "Requirement already satisfied: news-please in /usr/local/lib/python3.6/dist-packages (1.5.3)\n",
            "Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.6/dist-packages (from news-please) (2.0.5)\n",
            "Requirement already satisfied: langdetect>=1.0.7 in /usr/local/lib/python3.6/dist-packages (from news-please) (1.0.8)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (from news-please) (0.0.1)\n",
            "Requirement already satisfied: ago>=0.0.9 in /usr/local/lib/python3.6/dist-packages (from news-please) (0.0.93)\n",
            "Requirement already satisfied: beautifulsoup4>=4.3.2 in /usr/local/lib/python3.6/dist-packages (from news-please) (4.6.3)\n",
            "Requirement already satisfied: hurry.filesize>=0.9 in /usr/local/lib/python3.6/dist-packages (from news-please) (0.9)\n",
            "Requirement already satisfied: hjson>=1.5.8 in /usr/local/lib/python3.6/dist-packages (from news-please) (3.0.1)\n",
            "Requirement already satisfied: warcio>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from news-please) (1.7.4)\n",
            "Requirement already satisfied: lxml>=3.3.5 in /usr/local/lib/python3.6/dist-packages (from news-please) (4.2.6)\n",
            "Requirement already satisfied: newspaper3k>=0.2.8 in /usr/local/lib/python3.6/dist-packages (from news-please) (0.2.8)\n",
            "Requirement already satisfied: plac>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from news-please) (1.1.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from news-please) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from news-please) (2.8.1)\n",
            "Requirement already satisfied: awscli>=1.11.117 in /usr/local/lib/python3.6/dist-packages (from news-please) (1.18.118)\n",
            "Requirement already satisfied: elasticsearch>=2.4 in /usr/local/lib/python3.6/dist-packages (from news-please) (7.8.1)\n",
            "Requirement already satisfied: psycopg2-binary>=2.8.4 in /usr/local/lib/python3.6/dist-packages (from news-please) (2.8.5)\n",
            "Requirement already satisfied: readability-lxml>=0.6.2 in /usr/local/lib/python3.6/dist-packages (from news-please) (0.8.1)\n",
            "Requirement already satisfied: dotmap>=1.2.17 in /usr/local/lib/python3.6/dist-packages (from news-please) (1.3.17)\n",
            "Requirement already satisfied: Scrapy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from news-please) (2.3.0)\n",
            "Requirement already satisfied: PyMySQL>=0.7.9 in /usr/local/lib/python3.6/dist-packages (from news-please) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from hurry.filesize>=0.9->news-please) (49.2.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (3.13)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (0.3)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (1.1.0)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (0.35.1)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (5.2.1)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (3.2.5)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (7.0.0)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (2.23.0)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (2.2.3)\n",
            "Requirement already satisfied: botocore==1.17.41 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.117->news-please) (1.17.41)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.117->news-please) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.117->news-please) (0.15.2)\n",
            "Requirement already satisfied: rsa<=4.5.0,>=3.1.2 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.117->news-please) (4.5)\n",
            "Requirement already satisfied: colorama<0.4.4,>=0.2.5 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.117->news-please) (0.4.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from elasticsearch>=2.4->news-please) (2020.6.20)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from elasticsearch>=2.4->news-please) (1.24.3)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.6/dist-packages (from readability-lxml>=0.6.2->news-please) (3.0.4)\n",
            "Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (0.1.0)\n",
            "Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (1.0.2)\n",
            "Requirement already satisfied: pyOpenSSL>=16.2.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (19.1.0)\n",
            "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (3.0)\n",
            "Requirement already satisfied: Twisted>=17.9.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (20.3.0)\n",
            "Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (1.6.0)\n",
            "Requirement already satisfied: zope.interface>=4.1.3 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (5.1.0)\n",
            "Requirement already satisfied: service-identity>=16.0.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (18.1.0)\n",
            "Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (1.5.0)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (1.22.0)\n",
            "Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (0.1.16)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k>=0.2.8->news-please) (2.10)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k>=0.2.8->news-please) (1.5.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from botocore==1.17.41->awscli>=1.11.117->news-please) (0.10.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<=4.5.0,>=3.1.2->awscli>=1.11.117->news-please) (0.4.8)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.0->Scrapy>=1.1.0->news-please) (1.14.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->Scrapy>=1.1.0->news-please) (19.3.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->Scrapy>=1.1.0->news-please) (15.1.0)\n",
            "Requirement already satisfied: PyHamcrest!=1.10.0,>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->Scrapy>=1.1.0->news-please) (2.0.2)\n",
            "Requirement already satisfied: Automat>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->Scrapy>=1.1.0->news-please) (20.2.0)\n",
            "Requirement already satisfied: incremental>=16.10.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->Scrapy>=1.1.0->news-please) (17.5.0)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->Scrapy>=1.1.0->news-please) (20.0.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->Scrapy>=1.1.0->news-please) (0.2.8)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->Scrapy>=1.1.0->news-please) (2.20)\n",
            "Requirement already satisfied: google in /usr/local/lib/python3.6/dist-packages (2.0.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from google) (4.6.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oWOzCQAdm7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##utils.py\n",
        "\n",
        "import pandas as pd, numpy as np\n",
        "try:\n",
        "\timport bert\n",
        "except:\n",
        "\tprint(\"bert-for-tf2 not installed\")\n",
        "\n",
        "# transforms sentences to ids, masks and segment ids prepared to feed bert\n",
        "def convert_sentence_to_features(sentence, tokenizer, max_seq_len):\n",
        "    tokens = ['[CLS]']\n",
        "    tokens.extend(tokenizer.tokenize(sentence))\n",
        "    if len(tokens) > max_seq_len-1:\n",
        "        tokens = tokens[:max_seq_len-1]\n",
        "    tokens.append('[SEP]')\n",
        "    \n",
        "    segment_ids = [0] * len(tokens)\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    #Zero Mask till seq_length\n",
        "    zero_mask = [0] * (max_seq_len-len(tokens))\n",
        "    input_ids.extend(zero_mask)\n",
        "    input_mask.extend(zero_mask)\n",
        "    segment_ids.extend(zero_mask)\n",
        "    \n",
        "    return input_ids, input_mask, segment_ids\n",
        "\n",
        "def convert_sentences_to_features(sentences, tokenizer, max_seq_len=200):\n",
        "    all_input_ids = []\n",
        "    all_input_mask = []\n",
        "    all_segment_ids = []\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        input_ids, input_mask, segment_ids = convert_sentence_to_features(sentence, tokenizer, max_seq_len)\n",
        "        all_input_ids.append(input_ids)\n",
        "        all_input_mask.append(input_mask)\n",
        "        all_segment_ids.append(segment_ids)\n",
        "    \n",
        "    return all_input_ids, all_input_mask, all_segment_ids\n",
        "\n",
        "def generate_data_for_tokenizer(split_text,target_series):\n",
        "    labels_list = []\n",
        "    dates = []\n",
        "    for date, arrays in split_text.itertuples():\n",
        "        dates.extend([date]* len(arrays))\n",
        "    for date in dates:\n",
        "        labels_list.append(target_series.loc[date])\n",
        "    \n",
        "    split_text_flat = split_text.values.flatten()\n",
        "    sentence_list = [sentence for array in split_text_flat for sentence in array]\n",
        "    \n",
        "    labels = pd.DataFrame(labels_list, index = dates)\n",
        "    sentences  = pd.DataFrame(sentence_list, index = dates)\n",
        "    return sentences, labels\n",
        "\n",
        "# given an input text and a set of keywords, returns the top_n_terms which contain any of the keywords by frequency of appearance.\n",
        "def find_new_token_with_custom_keywords(array_of_text, custom_keywords, top_n_terms, extra_tokens):\n",
        "    \n",
        "    def contains_keyword(word,keywords):\n",
        "        for k in keywords:\n",
        "            if word.find(k) >= 0:\n",
        "                return True\n",
        "        return False\n",
        "    \n",
        "    def count_frequency(my_list): \n",
        "        freq = {} \n",
        "        for item in my_list: \n",
        "            if (item in freq): \n",
        "                freq[item] += 1\n",
        "            else: \n",
        "                freq[item] = 1\n",
        "        return freq\n",
        "    \n",
        "    raw_text = \"\".join(array_of_text).replace(\".com\",\"-com\").replace(\".\", \"\").replace(\",\", \"\").replace(\"\\n\", \" \").replace(\"-com\",\".com\")\n",
        "    raw_words = raw_text.split(\" \")\n",
        "    matches = []\n",
        "    for word in raw_words:\n",
        "        if contains_keyword(word.lower(),custom_keywords):\n",
        "            matches.append(word.lower())\n",
        "    \n",
        "    matches_count = count_frequency(matches)\n",
        "    #sorts the counts\n",
        "    #matches_dict = {k: v for k, v in sorted(matches_count.items(), key=lambda item: item[1], reverse = True)}\n",
        "    # selects top n words from the list\n",
        "    #new_tokens = list(matches_dict)[:top_n_terms]  + extra_tokens\n",
        "    import operator\n",
        "    sorted_x = sorted(matches_count.items(), key=operator.itemgetter(1), reverse = True)\n",
        "    new_tokens = [ tup[0] for tup in sorted_x[:top_n_terms]]  + extra_tokens\n",
        "    \n",
        "    print(\"New tokens to be added: \",new_tokens)\n",
        "    return new_tokens\n",
        "\n",
        "# creates bert tokenizer\n",
        "def create_tokenizer(vocab_file='vocab.txt', do_lower_case=True):\n",
        "    return bert.bert_tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "# appends extra tokens to the vocab of the tokenizer\n",
        "def add_new_tokens(new_vocab, tokenizer):\n",
        "    for i in range(len(new_vocab)):\n",
        "        new_key = new_vocab[i]\n",
        "        old_key = \"[unused{}]\".format(i)\n",
        "        value = tokenizer.vocab.pop(old_key)\n",
        "        tokenizer.vocab[new_key] = value\n",
        "    return tokenizer\n",
        "\n",
        "# transforms bet output in one continuous series removing the padding\n",
        "def bert_output_to_one_time_series_per_day(bert_inputs, bert_output, sentences):\n",
        "    n_sentences = sentences.groupby(sentences.index).count()\n",
        "    n_tokens = bert_inputs[\"input_mask\"][:].sum(axis = 1)\n",
        "    mask_out = [bert_output[1][counter,:length,:] for length, counter in zip(n_tokens,range(len(n_tokens)))]\n",
        "    \n",
        "    articles_per_day = []\n",
        "    acc = 0\n",
        "    for n in n_sentences.values:\n",
        "        n = n[0]\n",
        "        concat_articles = np.array(mask_out[acc:acc + n])\n",
        "        flattened = []\n",
        "        for sentence in concat_articles:\n",
        "            for token in sentence:\n",
        "                flattened.append(token)\n",
        "        flattened = np.array(flattened)\n",
        "        #flattened = np.array([token for token in sentence for sentence in concat_articles])\n",
        "        articles_per_day.append(flattened)\n",
        "        acc += n\n",
        "    return np.array(articles_per_day)\n",
        "\n",
        "# prepares_labels\n",
        "def label_transformer(prices, mode = \"returns\", shift = 5, index = None, standarized = False):\n",
        "    prices = pd.DataFrame(prices)\n",
        "    prices.columns = [\"today\"]\n",
        "    if index is not None:\n",
        "        prices = prices[index]\n",
        "    prices[\"tomorrow\"] = prices.shift(1)\n",
        "    prices[\"returns\"] = prices[\"today\"].pct_change()\n",
        "    prices[\"diff\"] = prices[\"today\"] - prices[\"tomorrow\"]\n",
        "    def standard(df):\n",
        "        return (df - df.mean())/df.std()\n",
        "    output = prices[mode].shift(shift).dropna()\n",
        "    return output if not standarized else standard(output)\n",
        "\n",
        "# computes and returns intersection among series\n",
        "def series_intersection(a,b):\n",
        "    a.index = pd.DatetimeIndex(a.index)\n",
        "    b.index = pd.DatetimeIndex(b.index)\n",
        "    intersection = pd.DatetimeIndex([value for value in a.index if value in b.index])\n",
        "    return a.loc[intersection], b.loc[intersection]\n",
        "\n",
        "\n",
        "\n",
        "def rolling_window_bert_2nd_dim(a, window):\n",
        "    shape = (a.shape[0] - window + 1, window, a.shape[1])\n",
        "    strides = (a.strides[0], a.strides[1]*a.shape[1], a.strides[1])\n",
        "    #print(shape, strides)\n",
        "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
        "\n",
        "\n",
        "def dummy():\n",
        "\treturn \"dah\""
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx1uhty9fHtR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "class Classifier_INCEPTION:\n",
        "\n",
        "    def __init__(self, output_directory, input_shape, nb_classes, verbose=False, build=True, batch_size=64,\n",
        "                 nb_filters=32, use_residual=True, use_bottleneck=True, depth=6, kernel_size=41, nb_epochs=1500):\n",
        "\n",
        "        self.output_directory = output_directory\n",
        "\n",
        "        self.nb_filters = nb_filters\n",
        "        self.use_residual = use_residual\n",
        "        self.use_bottleneck = use_bottleneck\n",
        "        self.depth = depth\n",
        "        self.kernel_size = kernel_size - 1\n",
        "        self.callbacks = None\n",
        "        self.batch_size = batch_size\n",
        "        self.bottleneck_size = 32\n",
        "        self.nb_epochs = nb_epochs\n",
        "\n",
        "        if build == True:\n",
        "            self.model = self.build_model(input_shape, nb_classes)\n",
        "            if (verbose == True):\n",
        "                self.model.summary()\n",
        "            self.verbose = verbose\n",
        "            self.model.save_weights(self.output_directory + 'model_init.hdf5')\n",
        "\n",
        "    def _inception_module(self, input_tensor, stride=1, activation='linear'):\n",
        "\n",
        "        if self.use_bottleneck and int(input_tensor.shape[-1]) > 1:\n",
        "            input_inception = keras.layers.Conv1D(filters=self.bottleneck_size, kernel_size=1,\n",
        "                                                  padding='same', activation=activation, use_bias=False)(input_tensor)\n",
        "        else:\n",
        "            input_inception = input_tensor\n",
        "\n",
        "        # kernel_size_s = [3, 5, 8, 11, 17]\n",
        "        kernel_size_s = [self.kernel_size // (2 ** i) for i in range(3)]\n",
        "\n",
        "        conv_list = []\n",
        "\n",
        "        for i in range(len(kernel_size_s)):\n",
        "            conv_list.append(keras.layers.Conv1D(filters=self.nb_filters, kernel_size=kernel_size_s[i],\n",
        "                                                 strides=stride, padding='same', activation=activation, use_bias=False)(\n",
        "                input_inception))\n",
        "\n",
        "        max_pool_1 = keras.layers.MaxPool1D(pool_size=3, strides=stride, padding='same')(input_tensor)\n",
        "\n",
        "        conv_6 = keras.layers.Conv1D(filters=self.nb_filters, kernel_size=1,\n",
        "                                     padding='same', activation=activation, use_bias=False)(max_pool_1)\n",
        "\n",
        "        conv_list.append(conv_6)\n",
        "\n",
        "        x = keras.layers.Concatenate(axis=2)(conv_list)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "        x = keras.layers.Activation(activation='relu')(x)\n",
        "        return x\n",
        "\n",
        "    def _shortcut_layer(self, input_tensor, out_tensor):\n",
        "        shortcut_y = keras.layers.Conv1D(filters=int(out_tensor.shape[-1]), kernel_size=1,\n",
        "                                         padding='same', use_bias=False)(input_tensor)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "        x = keras.layers.Add()([shortcut_y, out_tensor])\n",
        "        x = keras.layers.Activation('relu')(x)\n",
        "        return x\n",
        "\n",
        "    def build_layer_structure(self, input_tensor):\n",
        "        x = input_tensor\n",
        "        input_res = input_layer\n",
        "\n",
        "        for d in range(self.depth):\n",
        "\n",
        "            x = self._inception_module(x)\n",
        "\n",
        "            if self.use_residual and d % 3 == 2:\n",
        "                x = self._shortcut_layer(input_tensor, x)\n",
        "                input_res = x\n",
        "\n",
        "        gap_layer = keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "\n",
        "        return  gap_layer\n",
        "\n",
        "\n",
        "    def build_model(self, input_shape, nb_classes):\n",
        "        input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "        x = input_layer\n",
        "        input_res = input_layer\n",
        "\n",
        "        for d in range(self.depth):\n",
        "\n",
        "            x = self._inception_module(x)\n",
        "\n",
        "            if self.use_residual and d % 3 == 2:\n",
        "                x = self._shortcut_layer(input_res, x)\n",
        "                input_res = x\n",
        "\n",
        "        gap_layer = keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "        output_layer = keras.layers.Dense(nb_classes, activation='softmax')(gap_layer)\n",
        "\n",
        "        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50,\n",
        "                                                      min_lr=0.0001)\n",
        "\n",
        "        file_path = self.output_directory + 'best_model.hdf5'\n",
        "\n",
        "        model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss',\n",
        "                                                           save_best_only=True)\n",
        "\n",
        "        self.callbacks = [reduce_lr, model_checkpoint]\n",
        "\n",
        "        return model\n",
        "\n",
        "    def fit(self, x_train, y_train, x_val, y_val, y_true, plot_test_acc=False):\n",
        "\n",
        "        # x_val and y_val are only used to monitor the test loss and NOT for training\n",
        "\n",
        "        if self.batch_size is None:\n",
        "            mini_batch_size = int(min(x_train.shape[0] / 10, 16))\n",
        "        else:\n",
        "            mini_batch_size = self.batch_size\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        if plot_test_acc:\n",
        "\n",
        "            hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=self.nb_epochs,\n",
        "                                  verbose=self.verbose, validation_data=(x_val, y_val), callbacks=self.callbacks)\n",
        "        else:\n",
        "\n",
        "            hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=self.nb_epochs,\n",
        "                                  verbose=self.verbose, callbacks=self.callbacks)\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        self.model.save(self.output_directory + 'last_model.hdf5')\n",
        "\n",
        "        y_pred = self.predict(x_val, y_true, x_train, y_train, y_val)\n",
        "\n",
        "        # save predictions\n",
        "        np.save(self.output_directory + 'y_pred.npy', y_pred)\n",
        "\n",
        "        # convert the predicted from binary to integer\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "        keras.backend.clear_session()\n",
        "\n",
        "        return hist\n",
        "\n",
        "    def predict(self, x_test, y_true, x_train, y_train, y_test):\n",
        "        start_time = time.time()\n",
        "        model_path = self.output_directory + 'best_model.hdf5'\n",
        "        model = keras.models.load_model(model_path)\n",
        "        y_pred = model.predict(x_test, batch_size=self.batch_size)\n",
        "        return y_pred"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOFMsxaTZK9V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "17367130-7058-48e5-e5d1-2fcd91d9c760"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "# !ls \"/content/drive/My Drive\""
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxg9lgNhZBQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd, numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from utils import *\n",
        "\n",
        "import importlib\n",
        "import utils\n",
        "importlib.reload(utils)\n",
        "from utils import *\n",
        "\n",
        "import yfinance as yf\n",
        "\n",
        "start = \"2019-01-01\"\n",
        "end   = \"2020-03-20\"\n",
        "\n",
        "bitcoin = False\n",
        "if not bitcoin:   \n",
        "    stocks = [\"TSLA\"]\n",
        "    keywords = {\"TSLA\": [\"Tesla\", \"Elon Musk\"]}\n",
        "else:    \n",
        "    stocks = [\"BTC-USD\"]\n",
        "    keywords = {\"BTC-USD\" : [\"Bitcoin\", \"Cryptocurrency\"] }\n",
        "\n",
        "df_financial = yf.download(stocks, \n",
        "                     #period = \"1Y\",\n",
        "                      start= start, \n",
        "                      end= end, \n",
        "                      progress=False)\n",
        "prices = df_financial[\"Close\"]\n",
        "prices = pd.DataFrame(data = prices, index = pd.date_range(start,end)).fillna(method = \"bfill\")\n",
        "\n",
        "df_bitcoin = pd.read_csv(\"/content/drive/My Drive/ColabData/articles_Bitcoin-Cryptocurrency_start=2019-01-01_end=2020-12-31.csv\", index_col = 0)\n",
        "df_tesla = pd.read_csv(\"/content/drive/My Drive/ColabData/articles_Tesla-Elon_Musk_start=2019-01-01_end=2020-12-31.csv\", index_col = 0)\n",
        "df = df_bitcoin if bitcoin else df_tesla\n",
        "\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caiPUUFkcEwe",
        "colab_type": "text"
      },
      "source": [
        "Split sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkbmlgJrcGmZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "94f31490-95ae-495e-a4dd-fb40c0cf050d"
      },
      "source": [
        "main_text = False\n",
        "if main_text:\n",
        "    # main_text\n",
        "    text = df.maintext #pd.DataFrame(df.maintext.values, index = df[\"date_google\"]).dropna()\n",
        "    text.index = df[\"date_google\"]\n",
        "else:    \n",
        "    # titles and descriptions\n",
        "    titles = df.title\n",
        "    titles.index = df[\"date_google\"]\n",
        "    descriptions = df.description\n",
        "    descriptions.index = df[\"date_google\"]\n",
        "    text = pd.concat([titles,descriptions]).sort_index().dropna()\n",
        "    \n",
        "print(text.values[0])\n",
        "\n",
        "import nltk.data\n",
        "nltk.download('punkt')\n",
        "split_sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "split_sentences = text.apply(split_sentence_tokenizer.tokenize)\n",
        "split_sentences = split_sentences.groupby(split_sentences.index).sum()\n",
        "split_sentences = pd.DataFrame(split_sentences)\n",
        "\n",
        "df_ret = label_transformer(prices.copy(), shift = 1)\n",
        "\n",
        "split_sentences, df_ret = series_intersection(split_sentences, df_ret)\n",
        "raw_sentences, raw_labels = generate_data_for_tokenizer(split_sentences,df_ret)\n",
        "\n",
        "lengths = raw_sentences.apply(lambda x: len(x[0].split()), axis = 1)\n",
        "sentences = raw_sentences[(lengths > 10) & (lengths < 120)] #filter short and long sentences\n",
        "labels = raw_labels[(lengths > 10) & (lengths < 120)]\n",
        "\n",
        "keywords_bitcoin = [\"crypto\", \"BTC\", \"bitcoin\", \"blockchain\"]\n",
        "keywords_tesla =   [\"tesla\", \"Elon\", \"Musk\", \"TSLA\"]\n",
        "keys = keywords_bitcoin if bitcoin else keywords_tesla\n",
        "raw_text = \"\".join(text.values).replace(\".com\",\"-com\").replace(\".\", \"\").replace(\",\", \"\").replace(\"\\n\", \" \").replace(\"-com\",\".com\")\n",
        "new_tokens = find_new_token_with_custom_keywords(raw_text, keys , 6, [])"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Conor McGregor compared himself to Elon Musk in fiery exchange with Floyd Mayweather's manager\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "New tokens to be added:  ['tesla', \"tesla's\", 'tesla’s', 'teslas', 'teslaelon', 'musktesla']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a77gW9KSd2od",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "48331dfe-0fe6-44c5-834d-d1ecd87c508f"
      },
      "source": [
        "#model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\", from_pt=True)\n",
        "tokenizer.add_tokens(new_tokens)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFBertForSequenceClassification were initialized from the TF 2.0 model.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Os2JgdSeVsh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4e8fda7a-ca65-4de2-de36-bc69f4e95ec0"
      },
      "source": [
        "example = \"Bitcoin futures are trading below the cryptocurrency's spot price\"\n",
        "print(tokenizer.tokenize(example))\n",
        "example_ids = tokenizer.encode(example)\n",
        "print(example_ids)\n",
        "output = model.predict([example_ids]) \n",
        "print(\"    1 star     2 stars     3 stars     4 stars     5 stars\")\n",
        "print(output[0])"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['bit', '##co', '##in', 'futures', 'are', 'trading', 'below', 'the', 'cry', '##pt', '##oc', '##urre', '##ncy', \"'\", 's', 'spot', 'price']\n",
            "[101, 16464, 10805, 10262, 42272, 10320, 34948, 16934, 10103, 29917, 15903, 20731, 46642, 19771, 112, 161, 24311, 16993, 102]\n",
            "    1 star     2 stars     3 stars     4 stars     5 stars\n",
            "[[ 1.1589032   0.6475529   0.23367804 -0.6871393  -1.1347752 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0-vmro8efP2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "fd8ae1cb-36bf-429c-bbf3-7b4991dc978e"
      },
      "source": [
        "example = \"I am so disappointed with this product\"\n",
        "print(tokenizer.tokenize(example))\n",
        "example_ids = tokenizer.encode(example)\n",
        "print(example_ids)\n",
        "output = model.predict([example_ids])\n",
        "print(\"    1 star     2 stars    3 stars    4 stars    5 stars\")\n",
        "print(output[0])"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'am', 'so', 'disa', '##ppo', '##inted', 'with', 'this', 'product']\n",
            "[101, 151, 10345, 10297, 31021, 54894, 83912, 10171, 10372, 20058, 102]\n",
            "    1 star     2 stars    3 stars    4 stars    5 stars\n",
            "[[ 3.3838353  2.8609188  0.6469802 -2.6131783 -3.5148158]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBXhyvj6elaM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "f5e26b92-9acf-4115-a91c-d4308ae11198"
      },
      "source": [
        "encoded_sentences = sentences[0].apply(lambda x : tokenizer.encode(x))\n",
        "print (sentences[0])\n",
        "print (encoded_sentences)\n",
        "print (encoded_sentences[0])\n",
        "print (encoded_sentences[1])\n",
        "encoded_sentences.shape"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-01-03    Tesla CEO Elon Musk doubled down on criticism ...\n",
            "2019-01-03    It’s a counterintuitive result to mention toda...\n",
            "2019-01-03    But over a century of research on work environ...\n",
            "2019-01-03    Elon Musk tweeted that Singapore has been unwe...\n",
            "2019-01-03    From a Japanese mathematician to Elon Musk, he...\n",
            "                                    ...                        \n",
            "2020-03-19    Tesla Cybertruck Gigafactory: Oklahoma pulls o...\n",
            "2020-03-19    NYC mayor asks Elon Musk to manufacture ventil...\n",
            "2020-03-19    Elon Musk downplays coronavirus as Tesla facto...\n",
            "2020-03-19    Elon Musk took to Twitter yesterday to claim h...\n",
            "2020-03-19    The need is clear, let's hope he comes through...\n",
            "Name: 0, Length: 3186, dtype: object\n",
            "2019-01-03    [101, 51571, 23693, 21834, 10115, 23139, 10167...\n",
            "2019-01-03    [101, 10197, 100, 161, 143, 32964, 16790, 1754...\n",
            "2019-01-03    [101, 10502, 10323, 143, 11516, 10108, 11865, ...\n",
            "2019-01-03    [101, 21834, 10115, 23139, 10167, 12915, 11894...\n",
            "2019-01-03    [101, 10195, 143, 14201, 33508, 17291, 95605, ...\n",
            "                                    ...                        \n",
            "2020-03-19    [101, 51571, 69742, 32831, 11732, 21464, 10547...\n",
            "2020-03-19    [101, 95561, 12263, 51329, 21834, 10115, 23139...\n",
            "2020-03-19    [101, 21834, 10115, 23139, 10167, 12090, 64653...\n",
            "2020-03-19    [101, 21834, 10115, 23139, 10167, 12384, 10114...\n",
            "2020-03-19    [101, 10103, 15415, 10127, 20674, 117, 12421, ...\n",
            "Name: 0, Length: 3186, dtype: object\n",
            "[101, 51571, 23693, 21834, 10115, 23139, 10167, 13965, 10163, 12090, 10125, 36103, 10108, 19649, 10125, 63585, 117, 22811, 10203, 10103, 11409, 10575, 118, 10652, 10438, 10662, 107, 10119, 24414, 21567, 10111, 107, 10114, 10235, 12485, 53030, 119, 102]\n",
            "[101, 10197, 100, 161, 143, 32964, 16790, 17540, 12899, 14712, 10114, 29583, 13980, 117, 19772, 10104, 143, 11828, 10203, 92306, 10107, 143, 35563, 118, 10103, 118, 29061, 118, 16256, 117, 11497, 118, 11573, 118, 10855, 118, 26987, 23314, 12705, 119, 102]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3186,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy6dMkyPiDLZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "1cc899ba-b073-4ed3-9a4d-0a84e3d4a2c5"
      },
      "source": [
        "print (encoded_sentences[0])\n",
        "print (model.predict([encoded_sentences[0]]))\n",
        "print (model.predict([encoded_sentences[0]])[0])\n",
        "\n",
        "print (sentences[0][3])\n",
        "print (sentences[0][4])\n",
        "print (sentences[0][5])\n",
        "print (sentences[0][6])\n",
        "print (sentences[0][7])\n",
        "print (sentences[0][9])\n",
        "print (sentences[0][10])\n",
        "print (sentences[0][16])\n",
        "print (sentences[0][17])\n",
        "print (sentences[0][18])\n",
        "print (sentences[0][19])\n",
        "print (sentences[0][20])\n",
        "\n",
        "# print (sentences[0][18])\n",
        "# print (encoded_sentences[18])\n",
        "# print (model.predict([encoded_sentences[18]]))\n",
        "# print (model.predict([encoded_sentences[18]])[0])\n",
        "\n",
        "print (sentences[0][10])\n",
        "print (encoded_sentences[10])\n",
        "# print (model.predict([encoded_sentences[10]]))\n",
        "# print (model.predict([encoded_sentences[10]])[0])\n",
        "\n",
        "print (sentences[0][19])\n",
        "print (encoded_sentences[19])\n",
        "# print (model.predict([encoded_sentences[19]]))\n",
        "# print (model.predict([encoded_sentences[19]])[0])\n",
        "\n",
        "# for i in range(12, len(encoded_sentences)): \n",
        "\n",
        "#     print (i)\n",
        "#     print(encoded_sentences[i]) \n",
        "#     output = model.predict([encoded_sentences[i]])[0]\n",
        "#     print (output)\n",
        "\n",
        "\n"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[101, 51571, 23693, 21834, 10115, 23139, 10167, 13965, 10163, 12090, 10125, 36103, 10108, 19649, 10125, 63585, 117, 22811, 10203, 10103, 11409, 10575, 118, 10652, 10438, 10662, 107, 10119, 24414, 21567, 10111, 107, 10114, 10235, 12485, 53030, 119, 102]\n",
            "(array([[ 1.313773  ,  0.8246941 ,  0.26150295, -0.6092757 , -1.4050335 ]],\n",
            "      dtype=float32),)\n",
            "[[ 1.313773    0.8246941   0.26150295 -0.6092757  -1.4050335 ]]\n",
            "Elon Musk tweeted that Singapore has been unwelcoming to Tesla Inc., adding to his previous assertions that the government doesn’t support electric vehicles.\n",
            "From a Japanese mathematician to Elon Musk, here are the people who have been linked to Satoshi Nakamoto.\n",
            "Working extremely long hours doesn't make you better at your job.\n",
            "From Dorian Nakamoto to Elon Musk: The Incomplete List of People Speculated to Be Satoshi Nakamoto\n",
            "Tesla Model 3: Elon Musk's EV Just Got a Surprise Boost on Battery Range\n",
            "Tesla shares tumbled nearly 10 percent Wednesday and Thursday — despite closing out 2018 with record sales and production numbers.\n",
            "Investors appeared to be focusing more on potential roadblocks, including a flood of new competition and the phase-out of the federal tax credits that offset the cost of Tesla's battery-electric vehicles.\n",
            "'This is going to get ugly': Azealia Banks ramps up public feud with Elon Musk\n",
            "Tesla shares had another rough week following the release of disappoinging Q4 delivery results.\n",
            "Elon Musk Vs. Clayton Christensen: A Look At The Nature Of Disruption\n",
            "Elon Musk's 'Blastar' would be a perfect addition to Tesla's Easter Eggs\n",
            "Clayton Christensen is a big name when it comes to studying innovation and disruption.\n",
            "Investors appeared to be focusing more on potential roadblocks, including a flood of new competition and the phase-out of the federal tax credits that offset the cost of Tesla's battery-electric vehicles.\n",
            "[101, 67446, 14889, 10114, 10346, 67493, 10772, 10125, 21570, 11925, 31809, 21906, 10107, 117, 11371, 143, 40241, 10108, 10246, 14262, 10110, 10103, 17324, 118, 10871, 10108, 10103, 12501, 22389, 33896, 10203, 47117, 10337, 10103, 18153, 10108, 105879, 34794, 118, 15988, 25327, 119, 102]\n",
            "Elon Musk's 'Blastar' would be a perfect addition to Tesla's Easter Eggs\n",
            "[101, 21834, 10115, 23139, 10167, 112, 161, 112, 47732, 10370, 112, 11008, 10346, 143, 23021, 15000, 10114, 105879, 58776, 48540, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL49QLltqP-G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "f4c10a62-f93e-4ac3-ee19-6fb3effe1cea"
      },
      "source": [
        "predictions = encoded_sentences.apply(lambda x : model.predict([x])[0])\n",
        "predictions.shape\n",
        "avg_predictions.shape"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-cbb72fdf3648>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mavg_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3848\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-cbb72fdf3648>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mavg_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1597\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  indices[0,36] = 105879 is not in [0, 105879)\n\t [[node tf_bert_for_sequence_classification_2/bert/embeddings/Gather (defined at /usr/local/lib/python3.6/dist-packages/transformers/modeling_tf_bert.py:188) ]] [Op:__inference_predict_function_45034]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node tf_bert_for_sequence_classification_2/bert/embeddings/Gather:\n IteratorGetNext (defined at <ipython-input-45-4c1dd4675664>:2)\n\nFunction call stack:\npredict_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBK2XUNXf0Ly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import LSTM, Dense, Input, Flatten\n",
        "from tensorflow.keras.layers import TimeDistributed, Dropout, BatchNormalization, MaxPooling1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "def diff2p(d, start):\n",
        "    prices = []\n",
        "    cum_diff = start\n",
        "    for diff in d:\n",
        "        prices.append(diff + cum_diff)\n",
        "        cum_diff += diff\n",
        "    return prices\n",
        "\n",
        "def r2p(d, start = 100):\n",
        "    return start * (1 + d).cumprod()\n",
        "\n",
        "def rolling_window(a, window, step_size):\n",
        "    shape = a.shape[:-1] + (a.shape[-1] - window + 1 - step_size + 1, window)\n",
        "    strides = a.strides + (a.strides[-1] * step_size,)\n",
        "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
        "\n",
        "def rolling_window_bert_2nd_dim(a, window):\n",
        "    shape = (a.shape[0] - window + 1, window, a.shape[1])\n",
        "    strides = (a.strides[0], a.strides[1]*a.shape[1], a.strides[1])\n",
        "    #print(shape, strides)\n",
        "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
        "\n",
        "def lstm_model(n_steps):\n",
        "    n_features = 5\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(100, input_shape = (n_steps, n_features), return_sequences = True))\n",
        "    model.add(TimeDistributed(Dense(20, activation='elu')))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='elu'))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "def dense_model():\n",
        "    n_features = 5\n",
        "    model = Sequential()\n",
        "    #model.add(CuDNNLSTM(200, input_shape=(None, n_features), return_sequences= True))\n",
        "    model.add(Input((n_features)))\n",
        "    model.add(Dense(200))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(50, activation='tanh'))\n",
        "    model.add(Dense(1, activation='tanh'))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViZwpXzxgSkL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "ad0f6c2b-bcff-4092-a726-4f823e8fed43"
      },
      "source": [
        "mode = \"returns\"\n",
        "shift = 1\n",
        "df_prices = label_transformer(prices.copy(), shift = shift, mode = mode)\n",
        "avg_predictions, df_prices = series_intersection(avg_predictions, df_prices)\n",
        "\n",
        "threshold = \"2019-11-01\"\n",
        "X_train = avg_predictions.loc[:threshold]\n",
        "X_test = avg_predictions.loc[threshold:]\n",
        "y_train = df_prices.loc[:threshold]\n",
        "y_test = df_prices.loc[threshold:]\n",
        "\n",
        "\n",
        "model = dense_model()\n",
        "history = model.fit(X_train,y_train, validation_data=(X_test, y_test), epochs = 100)\n",
        "pred_tr = model.predict(X_train)\n",
        "pred_te = model.predict(X_test)\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-decaf500ae0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mshift\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_prices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mavg_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_prices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseries_intersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_prices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2019-11-01\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'avg_predictions' is not defined"
          ]
        }
      ]
    }
  ]
}