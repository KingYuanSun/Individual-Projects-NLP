{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOtBPNH9Duaekzhz48wKds/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KingYuanSun/Individual-Projects-NLP/blob/master/Sentiment_Analysis_StockTrendPrediction_Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zIXSKk5Y_DI",
        "colab_type": "text"
      },
      "source": [
        "BERT with Sentiment analysis¶\n",
        "In this notebook I used a pretrained version of BERT avaliable as a huggingface transformed to classify the sentiment of news articles about Bitcoin and Tesla, and applied an LSTM to predict the stock returns\n",
        "\n",
        "Sources\n",
        "https://towardsdatascience.com/fine-tuning-bert-with-keras-and-tf-module-ed24ea91cff2\n",
        "\n",
        "Bert for dummies: https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03\n",
        "Bert for long texts: https://medium.com/@armandj.olivares/using-bert-for-classifying-documents-with-long-texts-5c3e7b04573d\n",
        "googles notebook: https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb\n",
        "strong.io: https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b\n",
        "strong.io notebook : https://github.com/strongio/keras-bert\n",
        "https://keras.io/layers/writing-your-own-keras-layers/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOWNlneVaHc8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe4cd550-d12b-4de6-bb66-f683e38745da"
      },
      "source": [
        "!pip install utils\n",
        "!pip install bert-for-tf2\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install nltk\n",
        "!pip install yfinance\n",
        "!pip install news-please\n",
        "!pip install google\n",
        "!pip install transformers"
      ],
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: utils in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (0.14.5)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.9.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.5)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.18.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.6/dist-packages (0.1.54)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.6/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.18.5)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from yfinance) (0.0.9)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.0.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.24->yfinance) (1.15.0)\n",
            "Requirement already satisfied: news-please in /usr/local/lib/python3.6/dist-packages (1.5.3)\n",
            "Requirement already satisfied: hurry.filesize>=0.9 in /usr/local/lib/python3.6/dist-packages (from news-please) (0.9)\n",
            "Requirement already satisfied: Scrapy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from news-please) (2.3.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.3.2 in /usr/local/lib/python3.6/dist-packages (from news-please) (4.6.3)\n",
            "Requirement already satisfied: awscli>=1.11.117 in /usr/local/lib/python3.6/dist-packages (from news-please) (1.18.118)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from news-please) (1.15.0)\n",
            "Requirement already satisfied: elasticsearch>=2.4 in /usr/local/lib/python3.6/dist-packages (from news-please) (7.8.1)\n",
            "Requirement already satisfied: readability-lxml>=0.6.2 in /usr/local/lib/python3.6/dist-packages (from news-please) (0.8.1)\n",
            "Requirement already satisfied: langdetect>=1.0.7 in /usr/local/lib/python3.6/dist-packages (from news-please) (1.0.8)\n",
            "Requirement already satisfied: plac>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from news-please) (1.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from news-please) (2.8.1)\n",
            "Requirement already satisfied: warcio>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from news-please) (1.7.4)\n",
            "Requirement already satisfied: PyMySQL>=0.7.9 in /usr/local/lib/python3.6/dist-packages (from news-please) (0.10.0)\n",
            "Requirement already satisfied: psycopg2-binary>=2.8.4 in /usr/local/lib/python3.6/dist-packages (from news-please) (2.8.5)\n",
            "Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.6/dist-packages (from news-please) (2.0.5)\n",
            "Requirement already satisfied: ago>=0.0.9 in /usr/local/lib/python3.6/dist-packages (from news-please) (0.0.93)\n",
            "Requirement already satisfied: dotmap>=1.2.17 in /usr/local/lib/python3.6/dist-packages (from news-please) (1.3.17)\n",
            "Requirement already satisfied: newspaper3k>=0.2.8 in /usr/local/lib/python3.6/dist-packages (from news-please) (0.2.8)\n",
            "Requirement already satisfied: lxml>=3.3.5 in /usr/local/lib/python3.6/dist-packages (from news-please) (4.2.6)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (from news-please) (0.0.1)\n",
            "Requirement already satisfied: hjson>=1.5.8 in /usr/local/lib/python3.6/dist-packages (from news-please) (3.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from hurry.filesize>=0.9->news-please) (49.2.0)\n",
            "Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (1.5.0)\n",
            "Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (1.1.0)\n",
            "Requirement already satisfied: zope.interface>=4.1.3 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (5.1.0)\n",
            "Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (0.1.0)\n",
            "Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (0.1.16)\n",
            "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (3.0)\n",
            "Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (1.6.0)\n",
            "Requirement already satisfied: pyOpenSSL>=16.2.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (19.1.0)\n",
            "Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (1.0.2)\n",
            "Requirement already satisfied: Twisted>=17.9.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (20.3.0)\n",
            "Requirement already satisfied: service-identity>=16.0.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (18.1.0)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy>=1.1.0->news-please) (1.22.0)\n",
            "Requirement already satisfied: PyYAML<5.4,>=3.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.117->news-please) (3.13)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.117->news-please) (0.15.2)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.117->news-please) (0.3.3)\n",
            "Requirement already satisfied: rsa<=4.5.0,>=3.1.2 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.117->news-please) (4.5)\n",
            "Requirement already satisfied: colorama<0.4.4,>=0.2.5 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.117->news-please) (0.4.3)\n",
            "Requirement already satisfied: botocore==1.17.41 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.117->news-please) (1.17.41)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from elasticsearch>=2.4->news-please) (2020.6.20)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from elasticsearch>=2.4->news-please) (1.24.3)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.6/dist-packages (from readability-lxml>=0.6.2->news-please) (3.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (0.35.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (2.23.0)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (7.0.0)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (0.3)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (3.2.5)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (2.2.3)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (0.0.4)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (5.2.1)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.0->Scrapy>=1.1.0->news-please) (1.14.1)\n",
            "Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.6/dist-packages (from itemloaders>=1.0.1->Scrapy>=1.1.0->news-please) (0.10.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->Scrapy>=1.1.0->news-please) (15.1.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->Scrapy>=1.1.0->news-please) (19.3.0)\n",
            "Requirement already satisfied: PyHamcrest!=1.10.0,>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->Scrapy>=1.1.0->news-please) (2.0.2)\n",
            "Requirement already satisfied: Automat>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->Scrapy>=1.1.0->news-please) (20.2.0)\n",
            "Requirement already satisfied: incremental>=16.10.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->Scrapy>=1.1.0->news-please) (17.5.0)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->Scrapy>=1.1.0->news-please) (20.0.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->Scrapy>=1.1.0->news-please) (0.2.8)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->Scrapy>=1.1.0->news-please) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k>=0.2.8->news-please) (2.10)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k>=0.2.8->news-please) (1.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->Scrapy>=1.1.0->news-please) (2.20)\n",
            "Requirement already satisfied: google in /usr/local/lib/python3.6/dist-packages (2.0.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from google) (4.6.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oWOzCQAdm7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##utils.py\n",
        "\n",
        "import pandas as pd, numpy as np\n",
        "try:\n",
        "\timport bert\n",
        "except:\n",
        "\tprint(\"bert-for-tf2 not installed\")\n",
        "\n",
        "# transforms sentences to ids, masks and segment ids prepared to feed bert\n",
        "def convert_sentence_to_features(sentence, tokenizer, max_seq_len):\n",
        "    tokens = ['[CLS]']\n",
        "    tokens.extend(tokenizer.tokenize(sentence))\n",
        "    if len(tokens) > max_seq_len-1:\n",
        "        tokens = tokens[:max_seq_len-1]\n",
        "    tokens.append('[SEP]')\n",
        "    \n",
        "    segment_ids = [0] * len(tokens)\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    #Zero Mask till seq_length\n",
        "    zero_mask = [0] * (max_seq_len-len(tokens))\n",
        "    input_ids.extend(zero_mask)\n",
        "    input_mask.extend(zero_mask)\n",
        "    segment_ids.extend(zero_mask)\n",
        "    \n",
        "    return input_ids, input_mask, segment_ids\n",
        "\n",
        "def convert_sentences_to_features(sentences, tokenizer, max_seq_len=200):\n",
        "    all_input_ids = []\n",
        "    all_input_mask = []\n",
        "    all_segment_ids = []\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        input_ids, input_mask, segment_ids = convert_sentence_to_features(sentence, tokenizer, max_seq_len)\n",
        "        all_input_ids.append(input_ids)\n",
        "        all_input_mask.append(input_mask)\n",
        "        all_segment_ids.append(segment_ids)\n",
        "    \n",
        "    return all_input_ids, all_input_mask, all_segment_ids\n",
        "\n",
        "def generate_data_for_tokenizer(split_text,target_series):\n",
        "    labels_list = []\n",
        "    dates = []\n",
        "    for date, arrays in split_text.itertuples():\n",
        "        dates.extend([date]* len(arrays))\n",
        "    for date in dates:\n",
        "        labels_list.append(target_series.loc[date])\n",
        "    \n",
        "    split_text_flat = split_text.values.flatten()\n",
        "    sentence_list = [sentence for array in split_text_flat for sentence in array]\n",
        "    \n",
        "    labels = pd.DataFrame(labels_list, index = dates)\n",
        "    sentences  = pd.DataFrame(sentence_list, index = dates)\n",
        "    return sentences, labels\n",
        "\n",
        "# given an input text and a set of keywords, returns the top_n_terms which contain any of the keywords by frequency of appearance.\n",
        "def find_new_token_with_custom_keywords(array_of_text, custom_keywords, top_n_terms, extra_tokens):\n",
        "    \n",
        "    def contains_keyword(word,keywords):\n",
        "        for k in keywords:\n",
        "            if word.find(k) >= 0:\n",
        "                return True\n",
        "        return False\n",
        "    \n",
        "    def count_frequency(my_list): \n",
        "        freq = {} \n",
        "        for item in my_list: \n",
        "            if (item in freq): \n",
        "                freq[item] += 1\n",
        "            else: \n",
        "                freq[item] = 1\n",
        "        return freq\n",
        "    \n",
        "    raw_text = \"\".join(array_of_text).replace(\".com\",\"-com\").replace(\".\", \"\").replace(\",\", \"\").replace(\"\\n\", \" \").replace(\"-com\",\".com\")\n",
        "    raw_words = raw_text.split(\" \")\n",
        "    matches = []\n",
        "    for word in raw_words:\n",
        "        if contains_keyword(word.lower(),custom_keywords):\n",
        "            matches.append(word.lower())\n",
        "    \n",
        "    matches_count = count_frequency(matches)\n",
        "    #sorts the counts\n",
        "    #matches_dict = {k: v for k, v in sorted(matches_count.items(), key=lambda item: item[1], reverse = True)}\n",
        "    # selects top n words from the list\n",
        "    #new_tokens = list(matches_dict)[:top_n_terms]  + extra_tokens\n",
        "    import operator\n",
        "    sorted_x = sorted(matches_count.items(), key=operator.itemgetter(1), reverse = True)\n",
        "    new_tokens = [ tup[0] for tup in sorted_x[:top_n_terms]]  + extra_tokens\n",
        "    \n",
        "    print(\"New tokens to be added: \",new_tokens)\n",
        "    return new_tokens\n",
        "\n",
        "# creates bert tokenizer\n",
        "def create_tokenizer(vocab_file='vocab.txt', do_lower_case=True):\n",
        "    return bert.bert_tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "# appends extra tokens to the vocab of the tokenizer\n",
        "def add_new_tokens(new_vocab, tokenizer):\n",
        "    for i in range(len(new_vocab)):\n",
        "        new_key = new_vocab[i]\n",
        "        old_key = \"[unused{}]\".format(i)\n",
        "        value = tokenizer.vocab.pop(old_key)\n",
        "        tokenizer.vocab[new_key] = value\n",
        "    return tokenizer\n",
        "\n",
        "# transforms bet output in one continuous series removing the padding\n",
        "def bert_output_to_one_time_series_per_day(bert_inputs, bert_output, sentences):\n",
        "    n_sentences = sentences.groupby(sentences.index).count()\n",
        "    n_tokens = bert_inputs[\"input_mask\"][:].sum(axis = 1)\n",
        "    mask_out = [bert_output[1][counter,:length,:] for length, counter in zip(n_tokens,range(len(n_tokens)))]\n",
        "    \n",
        "    articles_per_day = []\n",
        "    acc = 0\n",
        "    for n in n_sentences.values:\n",
        "        n = n[0]\n",
        "        concat_articles = np.array(mask_out[acc:acc + n])\n",
        "        flattened = []\n",
        "        for sentence in concat_articles:\n",
        "            for token in sentence:\n",
        "                flattened.append(token)\n",
        "        flattened = np.array(flattened)\n",
        "        #flattened = np.array([token for token in sentence for sentence in concat_articles])\n",
        "        articles_per_day.append(flattened)\n",
        "        acc += n\n",
        "    return np.array(articles_per_day)\n",
        "\n",
        "# prepares_labels\n",
        "def label_transformer(prices, mode = \"returns\", shift = 5, index = None, standarized = False):\n",
        "    prices = pd.DataFrame(prices)\n",
        "    prices.columns = [\"today\"]\n",
        "    if index is not None:\n",
        "        prices = prices[index]\n",
        "    prices[\"tomorrow\"] = prices.shift(1)\n",
        "    prices[\"returns\"] = prices[\"today\"].pct_change()\n",
        "    prices[\"diff\"] = prices[\"today\"] - prices[\"tomorrow\"]\n",
        "    def standard(df):\n",
        "        return (df - df.mean())/df.std()\n",
        "    output = prices[mode].shift(shift).dropna()\n",
        "    return output if not standarized else standard(output)\n",
        "\n",
        "# computes and returns intersection among series\n",
        "def series_intersection(a,b):\n",
        "    a.index = pd.DatetimeIndex(a.index)\n",
        "    b.index = pd.DatetimeIndex(b.index)\n",
        "    intersection = pd.DatetimeIndex([value for value in a.index if value in b.index])\n",
        "    return a.loc[intersection], b.loc[intersection]\n",
        "\n",
        "\n",
        "\n",
        "def rolling_window_bert_2nd_dim(a, window):\n",
        "    shape = (a.shape[0] - window + 1, window, a.shape[1])\n",
        "    strides = (a.strides[0], a.strides[1]*a.shape[1], a.strides[1])\n",
        "    #print(shape, strides)\n",
        "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
        "\n",
        "\n",
        "def dummy():\n",
        "\treturn \"dah\""
      ],
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx1uhty9fHtR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "class Classifier_INCEPTION:\n",
        "\n",
        "    def __init__(self, output_directory, input_shape, nb_classes, verbose=False, build=True, batch_size=64,\n",
        "                 nb_filters=32, use_residual=True, use_bottleneck=True, depth=6, kernel_size=41, nb_epochs=1500):\n",
        "\n",
        "        self.output_directory = output_directory\n",
        "\n",
        "        self.nb_filters = nb_filters\n",
        "        self.use_residual = use_residual\n",
        "        self.use_bottleneck = use_bottleneck\n",
        "        self.depth = depth\n",
        "        self.kernel_size = kernel_size - 1\n",
        "        self.callbacks = None\n",
        "        self.batch_size = batch_size\n",
        "        self.bottleneck_size = 32\n",
        "        self.nb_epochs = nb_epochs\n",
        "\n",
        "        if build == True:\n",
        "            self.model = self.build_model(input_shape, nb_classes)\n",
        "            if (verbose == True):\n",
        "                self.model.summary()\n",
        "            self.verbose = verbose\n",
        "            self.model.save_weights(self.output_directory + 'model_init.hdf5')\n",
        "\n",
        "    def _inception_module(self, input_tensor, stride=1, activation='linear'):\n",
        "\n",
        "        if self.use_bottleneck and int(input_tensor.shape[-1]) > 1:\n",
        "            input_inception = keras.layers.Conv1D(filters=self.bottleneck_size, kernel_size=1,\n",
        "                                                  padding='same', activation=activation, use_bias=False)(input_tensor)\n",
        "        else:\n",
        "            input_inception = input_tensor\n",
        "\n",
        "        # kernel_size_s = [3, 5, 8, 11, 17]\n",
        "        kernel_size_s = [self.kernel_size // (2 ** i) for i in range(3)]\n",
        "\n",
        "        conv_list = []\n",
        "\n",
        "        for i in range(len(kernel_size_s)):\n",
        "            conv_list.append(keras.layers.Conv1D(filters=self.nb_filters, kernel_size=kernel_size_s[i],\n",
        "                                                 strides=stride, padding='same', activation=activation, use_bias=False)(\n",
        "                input_inception))\n",
        "\n",
        "        max_pool_1 = keras.layers.MaxPool1D(pool_size=3, strides=stride, padding='same')(input_tensor)\n",
        "\n",
        "        conv_6 = keras.layers.Conv1D(filters=self.nb_filters, kernel_size=1,\n",
        "                                     padding='same', activation=activation, use_bias=False)(max_pool_1)\n",
        "\n",
        "        conv_list.append(conv_6)\n",
        "\n",
        "        x = keras.layers.Concatenate(axis=2)(conv_list)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "        x = keras.layers.Activation(activation='relu')(x)\n",
        "        return x\n",
        "\n",
        "    def _shortcut_layer(self, input_tensor, out_tensor):\n",
        "        shortcut_y = keras.layers.Conv1D(filters=int(out_tensor.shape[-1]), kernel_size=1,\n",
        "                                         padding='same', use_bias=False)(input_tensor)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "        x = keras.layers.Add()([shortcut_y, out_tensor])\n",
        "        x = keras.layers.Activation('relu')(x)\n",
        "        return x\n",
        "\n",
        "    def build_layer_structure(self, input_tensor):\n",
        "        x = input_tensor\n",
        "        input_res = input_layer\n",
        "\n",
        "        for d in range(self.depth):\n",
        "\n",
        "            x = self._inception_module(x)\n",
        "\n",
        "            if self.use_residual and d % 3 == 2:\n",
        "                x = self._shortcut_layer(input_tensor, x)\n",
        "                input_res = x\n",
        "\n",
        "        gap_layer = keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "\n",
        "        return  gap_layer\n",
        "\n",
        "\n",
        "    def build_model(self, input_shape, nb_classes):\n",
        "        input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "        x = input_layer\n",
        "        input_res = input_layer\n",
        "\n",
        "        for d in range(self.depth):\n",
        "\n",
        "            x = self._inception_module(x)\n",
        "\n",
        "            if self.use_residual and d % 3 == 2:\n",
        "                x = self._shortcut_layer(input_res, x)\n",
        "                input_res = x\n",
        "\n",
        "        gap_layer = keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "        output_layer = keras.layers.Dense(nb_classes, activation='softmax')(gap_layer)\n",
        "\n",
        "        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50,\n",
        "                                                      min_lr=0.0001)\n",
        "\n",
        "        file_path = self.output_directory + 'best_model.hdf5'\n",
        "\n",
        "        model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss',\n",
        "                                                           save_best_only=True)\n",
        "\n",
        "        self.callbacks = [reduce_lr, model_checkpoint]\n",
        "\n",
        "        return model\n",
        "\n",
        "    def fit(self, x_train, y_train, x_val, y_val, y_true, plot_test_acc=False):\n",
        "\n",
        "        # x_val and y_val are only used to monitor the test loss and NOT for training\n",
        "\n",
        "        if self.batch_size is None:\n",
        "            mini_batch_size = int(min(x_train.shape[0] / 10, 16))\n",
        "        else:\n",
        "            mini_batch_size = self.batch_size\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        if plot_test_acc:\n",
        "\n",
        "            hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=self.nb_epochs,\n",
        "                                  verbose=self.verbose, validation_data=(x_val, y_val), callbacks=self.callbacks)\n",
        "        else:\n",
        "\n",
        "            hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=self.nb_epochs,\n",
        "                                  verbose=self.verbose, callbacks=self.callbacks)\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        self.model.save(self.output_directory + 'last_model.hdf5')\n",
        "\n",
        "        y_pred = self.predict(x_val, y_true, x_train, y_train, y_val)\n",
        "\n",
        "        # save predictions\n",
        "        np.save(self.output_directory + 'y_pred.npy', y_pred)\n",
        "\n",
        "        # convert the predicted from binary to integer\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "        keras.backend.clear_session()\n",
        "\n",
        "        return hist\n",
        "\n",
        "    def predict(self, x_test, y_true, x_train, y_train, y_test):\n",
        "        start_time = time.time()\n",
        "        model_path = self.output_directory + 'best_model.hdf5'\n",
        "        model = keras.models.load_model(model_path)\n",
        "        y_pred = model.predict(x_test, batch_size=self.batch_size)\n",
        "        return y_pred"
      ],
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOFMsxaTZK9V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fc18c0d0-4dcf-4326-84bd-95f4648cce07"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "# !ls \"/content/drive/My Drive\""
      ],
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxg9lgNhZBQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd, numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from utils import *\n",
        "\n",
        "import importlib\n",
        "import utils\n",
        "importlib.reload(utils)\n",
        "from utils import *\n",
        "\n",
        "import yfinance as yf\n",
        "\n",
        "start = \"2019-01-01\"\n",
        "end   = \"2020-03-20\"\n",
        "\n",
        "bitcoin = False\n",
        "if not bitcoin:   \n",
        "    stocks = [\"TSLA\"]\n",
        "    keywords = {\"TSLA\": [\"Tesla\", \"Elon Musk\"]}\n",
        "else:    \n",
        "    stocks = [\"BTC-USD\"]\n",
        "    keywords = {\"BTC-USD\" : [\"Bitcoin\", \"Cryptocurrency\"] }\n",
        "\n",
        "df_financial = yf.download(stocks, \n",
        "                     #period = \"1Y\",\n",
        "                      start= start, \n",
        "                      end= end, \n",
        "                      progress=False)\n",
        "prices = df_financial[\"Close\"]\n",
        "prices = pd.DataFrame(data = prices, index = pd.date_range(start,end)).fillna(method = \"bfill\")\n",
        "\n",
        "df_bitcoin = pd.read_csv(\"/content/drive/My Drive/ColabData/articles_Bitcoin-Cryptocurrency_start=2019-01-01_end=2020-12-31.csv\", index_col = 0)\n",
        "df_tesla = pd.read_csv(\"/content/drive/My Drive/ColabData/articles_Tesla-Elon_Musk_start=2019-01-01_end=2020-12-31.csv\", index_col = 0)\n",
        "df = df_bitcoin if bitcoin else df_tesla\n",
        "\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, AutoModelForSequenceClassification, TFBertForSequenceClassification \n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")"
      ],
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caiPUUFkcEwe",
        "colab_type": "text"
      },
      "source": [
        "Split sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkbmlgJrcGmZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "e3a220aa-fe9a-4577-dce9-e1b271ca4bd3"
      },
      "source": [
        "main_text = False\n",
        "if main_text:\n",
        "    # main_text\n",
        "    text = df.maintext #pd.DataFrame(df.maintext.values, index = df[\"date_google\"]).dropna()\n",
        "    text.index = df[\"date_google\"]\n",
        "else:    \n",
        "    # titles and descriptions\n",
        "    titles = df.title\n",
        "    titles.index = df[\"date_google\"]\n",
        "    descriptions = df.description\n",
        "    descriptions.index = df[\"date_google\"]\n",
        "    text = pd.concat([titles,descriptions]).sort_index().dropna()\n",
        "    \n",
        "print(text.values[0])\n",
        "\n",
        "import nltk.data\n",
        "nltk.download('punkt')\n",
        "split_sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "split_sentences = text.apply(split_sentence_tokenizer.tokenize)\n",
        "split_sentences = split_sentences.groupby(split_sentences.index).sum()\n",
        "split_sentences = pd.DataFrame(split_sentences)\n",
        "\n",
        "df_ret = label_transformer(prices.copy(), shift = 1)\n",
        "\n",
        "split_sentences, df_ret = series_intersection(split_sentences, df_ret)\n",
        "raw_sentences, raw_labels = generate_data_for_tokenizer(split_sentences,df_ret)\n",
        "\n",
        "lengths = raw_sentences.apply(lambda x: len(x[0].split()), axis = 1)\n",
        "sentences = raw_sentences[(lengths > 10) & (lengths < 120)] #filter short and long sentences\n",
        "\n",
        "labels = raw_labels[(lengths > 10) & (lengths < 120)]\n",
        "\n",
        "keywords_bitcoin = [\"crypto\", \"BTC\", \"bitcoin\", \"blockchain\"]\n",
        "keywords_tesla =   [\"tesla\", \"Elon\", \"Musk\", \"TSLA\", \"Tesla\"]\n",
        "keys = keywords_bitcoin if bitcoin else keywords_tesla\n",
        "raw_text = \"\".join(text.values).replace(\".com\",\"-com\").replace(\".\", \"\").replace(\",\", \"\").replace(\"\\n\", \" \").replace(\"-com\",\".com\")\n",
        "new_tokens = find_new_token_with_custom_keywords(raw_text, keys , 6, [])\n",
        "# new_tokens = new_tokens.append('tesla\\'s')\n",
        "\n",
        "print (new_tokens)\n",
        "print (sentences[0])\n",
        "print (sentences[0][19])\n",
        "\n",
        "print (sentences.dtypes)\n",
        "savedsentences = sentences\n",
        "\n",
        "res = [sub.replace('Tesla\\'s', 'tesla').replace('Tesla’s', 'tesla').replace('tesla\\'s', 'tesla').replace('tesla’s', 'tesla').replace('Teslas', 'tesla').replace('teslaelon', 'tesla').replace('musktesla', 'tesla') for sub in sentences[0]] \n",
        "\n",
        "# tesla’s Teslas\n",
        "\n",
        "sentences = pd.Series(res) \n",
        "print (sentences[19])\n",
        "print (sentences[103])\n",
        "# sentences = []\n",
        "# sentences.append(res)\n",
        "# print (sentences[0][19])\n",
        "# print (res[19])"
      ],
      "execution_count": 347,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Conor McGregor compared himself to Elon Musk in fiery exchange with Floyd Mayweather's manager\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "New tokens to be added:  ['tesla', \"tesla's\", 'tesla’s', 'teslas', 'teslaelon', 'musktesla']\n",
            "['tesla', \"tesla's\", 'tesla’s', 'teslas', 'teslaelon', 'musktesla']\n",
            "2019-01-03    Tesla CEO Elon Musk doubled down on criticism ...\n",
            "2019-01-03    It’s a counterintuitive result to mention toda...\n",
            "2019-01-03    But over a century of research on work environ...\n",
            "2019-01-03    Elon Musk tweeted that Singapore has been unwe...\n",
            "2019-01-03    From a Japanese mathematician to Elon Musk, he...\n",
            "                                    ...                        \n",
            "2020-03-19    Tesla Cybertruck Gigafactory: Oklahoma pulls o...\n",
            "2020-03-19    NYC mayor asks Elon Musk to manufacture ventil...\n",
            "2020-03-19    Elon Musk downplays coronavirus as Tesla facto...\n",
            "2020-03-19    Elon Musk took to Twitter yesterday to claim h...\n",
            "2020-03-19    The need is clear, let's hope he comes through...\n",
            "Name: 0, Length: 3186, dtype: object\n",
            "Elon Musk's 'Blastar' would be a perfect addition to Tesla's Easter Eggs\n",
            "0    object\n",
            "dtype: object\n",
            "Elon Musk's 'Blastar' would be a perfect addition to tesla Easter Eggs\n",
            "tesla customer referral program, in which Tesla owners can give their friends a referral code to get six months of free charging via tesla Supercharger network, will be ending on March 1st, Elon Musk said in a tweet.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a77gW9KSd2od",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f70e7b7c-8594-4976-d91c-5f3dccc7230e"
      },
      "source": [
        "#model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\", from_pt=True)\n",
        "tokenizer.add_tokens(new_tokens)"
      ],
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFBertForSequenceClassification were initialized from the TF 2.0 model.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 295
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Os2JgdSeVsh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "36fea2c8-aae8-4444-ab57-04aa712fe754"
      },
      "source": [
        "example = \"Bitcoin futures are trading below the cryptocurrency's spot price\"\n",
        "print(tokenizer.tokenize(example))\n",
        "example_ids = tokenizer.encode(example)\n",
        "print(example_ids)\n",
        "output = model.predict([example_ids]) \n",
        "print(\"    1 star     2 stars     3 stars     4 stars     5 stars\")\n",
        "print(output[0])"
      ],
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['bit', '##co', '##in', 'futures', 'are', 'trading', 'below', 'the', 'cry', '##pt', '##oc', '##urre', '##ncy', \"'\", 's', 'spot', 'price']\n",
            "[101, 16464, 10805, 10262, 42272, 10320, 34948, 16934, 10103, 29917, 15903, 20731, 46642, 19771, 112, 161, 24311, 16993, 102]\n",
            "    1 star     2 stars     3 stars     4 stars     5 stars\n",
            "[[ 1.1589032   0.6475529   0.23367804 -0.6871393  -1.1347752 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0-vmro8efP2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "39de0332-bb41-433f-8752-dd7ce4e11c85"
      },
      "source": [
        "example = \"I am so disappointed with this product\"\n",
        "print(tokenizer.tokenize(example))\n",
        "example_ids = tokenizer.encode(example)\n",
        "print(example_ids)\n",
        "output = model.predict([example_ids])\n",
        "print(\"    1 star     2 stars    3 stars    4 stars    5 stars\")\n",
        "print(output[0])"
      ],
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'am', 'so', 'disa', '##ppo', '##inted', 'with', 'this', 'product']\n",
            "[101, 151, 10345, 10297, 31021, 54894, 83912, 10171, 10372, 20058, 102]\n",
            "WARNING:tensorflow:5 out of the last 6377 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0b54abeb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "    1 star     2 stars    3 stars    4 stars    5 stars\n",
            "[[ 3.3838353  2.8609188  0.6469802 -2.6131783 -3.5148158]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTnZvF8p9fHD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "57595549-67a8-40b4-aba4-12a98dbb1bf0"
      },
      "source": [
        "example = \"Elon Musk's 'Blastar' would be a perfect addition to tesla Easter Eggs\"\n",
        "\n",
        "\n",
        "print(tokenizer.tokenize(example))\n",
        "example_ids = tokenizer.encode(example)\n",
        "print(example_ids)\n",
        "output = model.predict([example_ids])\n",
        "print(\"    1 star     2 stars    3 stars    4 stars    5 stars\")\n",
        "print(output[0])"
      ],
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['elo', '##n', 'mus', '##k', \"'\", 's', \"'\", 'blast', '##ar', \"'\", 'would', 'be', 'a', 'perfect', 'addition', 'to', 'tesla', 'easter', 'eggs']\n",
            "[101, 21834, 10115, 23139, 10167, 112, 161, 112, 47732, 10370, 112, 11008, 10346, 143, 23021, 15000, 10114, 51571, 58776, 48540, 102]\n",
            "WARNING:tensorflow:6 out of the last 6378 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0b54abeb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "    1 star     2 stars    3 stars    4 stars    5 stars\n",
            "[[-1.48236    -0.895975    0.41832617  1.1279131   0.57270813]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IksNea52Qwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = sentences.apply(lambda x : tokenizer.tokenize(x))"
      ],
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBXhyvj6elaM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "40a8f885-82c5-486d-ab5a-d4227f2d0cbe"
      },
      "source": [
        "encoded_sentences = sentences.apply(lambda x : tokenizer.encode(x))\n",
        "print (sentences)\n",
        "print (encoded_sentences)\n",
        "print (encoded_sentences[0])\n",
        "print (encoded_sentences[1])\n",
        "encoded_sentences.shape"
      ],
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       [tesla, ceo, elo, ##n, mus, ##k, double, ##d, ...\n",
            "1       [it, [UNK], s, a, counter, ##int, ##uit, ##ive...\n",
            "2       [but, over, a, century, of, research, on, work...\n",
            "3       [elo, ##n, mus, ##k, twee, ##ted, that, singap...\n",
            "4       [from, a, japanese, math, ##ema, ##tician, to,...\n",
            "                              ...                        \n",
            "3181    [tesla, cyber, ##tru, ##ck, gi, ##ga, ##fa, ##...\n",
            "3182    [nyc, mayor, asks, elo, ##n, mus, ##k, to, man...\n",
            "3183    [elo, ##n, mus, ##k, down, ##play, ##s, corona...\n",
            "3184    [elo, ##n, mus, ##k, took, to, twitter, yester...\n",
            "3185    [the, need, is, clear, ,, let, ', s, hope, he,...\n",
            "Length: 3186, dtype: object\n",
            "0       [101, 51571, 23693, 21834, 10115, 23139, 10167...\n",
            "1       [101, 10197, 100, 161, 143, 32964, 16790, 1754...\n",
            "2       [101, 10502, 10323, 143, 11516, 10108, 11865, ...\n",
            "3       [101, 21834, 10115, 23139, 10167, 12915, 11894...\n",
            "4       [101, 10195, 143, 14201, 33508, 17291, 95605, ...\n",
            "                              ...                        \n",
            "3181    [101, 51571, 69742, 32831, 11732, 21464, 10547...\n",
            "3182    [101, 95561, 12263, 51329, 21834, 10115, 23139...\n",
            "3183    [101, 21834, 10115, 23139, 10167, 12090, 64653...\n",
            "3184    [101, 21834, 10115, 23139, 10167, 12384, 10114...\n",
            "3185    [101, 10103, 15415, 10127, 20674, 117, 12421, ...\n",
            "Length: 3186, dtype: object\n",
            "[101, 51571, 23693, 21834, 10115, 23139, 10167, 13965, 10163, 12090, 10125, 36103, 10108, 19649, 10125, 63585, 117, 22811, 10203, 10103, 11409, 10575, 118, 10652, 10438, 10662, 107, 10119, 24414, 21567, 10111, 107, 10114, 10235, 12485, 53030, 119, 102]\n",
            "[101, 10197, 100, 161, 143, 32964, 16790, 17540, 12899, 14712, 10114, 29583, 13980, 117, 19772, 10104, 143, 11828, 10203, 92306, 10107, 143, 35563, 118, 10103, 118, 29061, 118, 16256, 117, 11497, 118, 11573, 118, 10855, 118, 26987, 23314, 12705, 119, 102]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3186,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 300
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy6dMkyPiDLZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "deea46a9-2d61-40e3-f35d-67755100245b"
      },
      "source": [
        "# if we have \"tesla's\", we failed\n",
        "\n",
        "# print (encoded_sentences[0])\n",
        "# print (model.predict([encoded_sentences[0]]))\n",
        "# print (model.predict([encoded_sentences[0]])[0])\n",
        "\n",
        "# print (sentences[0][3])\n",
        "# print (sentences[0][4])\n",
        "# print (sentences[0][5])\n",
        "# print (sentences[0][6])\n",
        "# print (sentences[0][7])\n",
        "# print (sentences[0][9])\n",
        "# print (sentences[0][10])\n",
        "# print (sentences[0][16])\n",
        "# print (sentences[0][17])\n",
        "# print (sentences[0][18])\n",
        "# print (sentences[0][19])\n",
        "# print (sentences[0][20])\n",
        "\n",
        "# print (sentences[0][18])\n",
        "# print (encoded_sentences[18])\n",
        "# print (model.predict([encoded_sentences[18]]))\n",
        "# print (model.predict([encoded_sentences[18]])[0])\n",
        "\n",
        "print (sentences[10])\n",
        "print (encoded_sentences[10])\n",
        "print (model.predict(encoded_sentences[10]))\n",
        "print (model.predict([encoded_sentences[10]])[0])\n",
        "\n",
        "print (sentences[19])\n",
        "print (encoded_sentences[19])\n",
        "print (model.predict([encoded_sentences[19]]))\n",
        "print (model.predict([encoded_sentences[19]])[0])\n",
        "\n",
        "# for i in range(1, len(encoded_sentences)): \n",
        "\n",
        "#     print (i)\n",
        "#     print (sentences[i])\n",
        "#     print(encoded_sentences[i]) \n",
        "#     output = model.predict([encoded_sentences[i]])[0]\n",
        "#     print (output)\n",
        "\n",
        "\n"
      ],
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['investors', 'appeared', 'to', 'be', 'focusing', 'more', 'on', 'potential', 'road', '##bl', '##ock', '##s', ',', 'including', 'a', 'flood', 'of', 'new', 'competition', 'and', 'the', 'phase', '-', 'out', 'of', 'the', 'federal', 'tax', 'credits', 'that', 'offs', '##et', 'the', 'cost', 'of', 'tesla', 'battery', '-', 'electric', 'vehicles', '.']\n",
            "[101, 67446, 14889, 10114, 10346, 67493, 10772, 10125, 21570, 11925, 31809, 21906, 10107, 117, 11371, 143, 40241, 10108, 10246, 14262, 10110, 10103, 17324, 118, 10871, 10108, 10103, 12501, 22389, 33896, 10203, 47117, 10337, 10103, 18153, 10108, 51571, 34794, 118, 15988, 25327, 119, 102]\n",
            "WARNING:tensorflow:7 out of the last 6379 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0b54abeb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "(array([[ 2.10614756e-01,  2.56466150e-01,  1.39126316e-01,\n",
            "        -2.67221451e-01, -1.65504664e-01],\n",
            "       [-4.38403189e-01,  5.69914579e-01,  7.56413877e-01,\n",
            "         2.80110668e-02, -6.12575054e-01],\n",
            "       [ 1.57692879e-01,  2.46165216e-01,  1.76105902e-01,\n",
            "        -2.54177243e-01, -1.54445529e-01],\n",
            "       [ 8.45648944e-02,  2.18766451e-01,  1.93011418e-01,\n",
            "        -2.12201223e-01, -1.44217163e-01],\n",
            "       [ 1.29186986e-02,  1.80537850e-01,  2.26959333e-01,\n",
            "        -1.69549808e-01, -1.11226961e-01],\n",
            "       [-8.91145468e-01, -1.72820419e-01,  6.02216661e-01,\n",
            "         4.37138468e-01,  1.00002326e-01],\n",
            "       [ 2.25782290e-01,  2.69848466e-01,  1.32788315e-01,\n",
            "        -2.79277653e-01, -1.77465022e-01],\n",
            "       [ 1.79127634e-01,  2.41989598e-01,  1.54304281e-01,\n",
            "        -2.54679471e-01, -1.46387428e-01],\n",
            "       [-9.81200952e-03,  1.78980425e-01,  2.71497607e-01,\n",
            "        -1.50241524e-01, -1.28665134e-01],\n",
            "       [ 1.26667172e-01,  2.30678022e-01,  1.82913259e-01,\n",
            "        -2.23044619e-01, -1.49072260e-01],\n",
            "       [ 2.31278375e-01,  2.61766970e-01,  1.31136537e-01,\n",
            "        -2.76586473e-01, -1.70786619e-01],\n",
            "       [ 2.52902955e-01,  2.75030583e-01,  1.23805419e-01,\n",
            "        -2.88558275e-01, -1.82458580e-01],\n",
            "       [-6.78583622e-01, -1.77490324e-01,  5.31619966e-01,\n",
            "         3.41869622e-01, -3.22050750e-02],\n",
            "       [ 2.12556213e-01,  2.63698339e-01,  1.41411498e-01,\n",
            "        -2.71481454e-01, -1.68973058e-01],\n",
            "       [ 1.94894895e-01,  2.57199913e-01,  1.48490950e-01,\n",
            "        -2.64713854e-01, -1.67217076e-01],\n",
            "       [ 2.03230083e-01,  2.57784098e-01,  1.40857592e-01,\n",
            "        -2.71474928e-01, -1.56926766e-01],\n",
            "       [ 1.17809504e-01,  2.55242437e-01,  1.94721207e-01,\n",
            "        -2.32384995e-01, -1.71897531e-01],\n",
            "       [ 2.14793697e-01,  2.64461756e-01,  1.35594741e-01,\n",
            "        -2.75538951e-01, -1.70973867e-01],\n",
            "       [ 1.74926415e-01,  2.58555412e-01,  1.58152193e-01,\n",
            "        -2.58889616e-01, -1.63912460e-01],\n",
            "       [-5.36126912e-01, -2.94747800e-02,  3.73748362e-01,\n",
            "         1.07825130e-01,  2.07239911e-02],\n",
            "       [ 2.98720840e-02,  1.87490165e-01,  2.01014638e-01,\n",
            "        -1.85672089e-01, -9.59026366e-02],\n",
            "       [ 2.13059455e-01,  2.59260416e-01,  1.40896425e-01,\n",
            "        -2.72425741e-01, -1.67875364e-01],\n",
            "       [-4.67417121e-01,  4.07968834e-02,  5.07152259e-01,\n",
            "         9.46606994e-02, -1.38397902e-01],\n",
            "       [ 2.27628097e-01,  2.65107095e-01,  1.32422641e-01,\n",
            "        -2.76434451e-01, -1.72262743e-01],\n",
            "       [ 1.99365422e-01,  2.56296754e-01,  1.41474292e-01,\n",
            "        -2.63058662e-01, -1.62857324e-01],\n",
            "       [ 2.14793697e-01,  2.64461756e-01,  1.35594741e-01,\n",
            "        -2.75538951e-01, -1.70973867e-01],\n",
            "       [ 2.13059455e-01,  2.59260416e-01,  1.40896425e-01,\n",
            "        -2.72425741e-01, -1.67875364e-01],\n",
            "       [ 2.06675440e-01,  2.62889504e-01,  1.39268741e-01,\n",
            "        -2.59814978e-01, -1.71965867e-01],\n",
            "       [-4.23671037e-01, -4.23058867e-02,  3.79003406e-01,\n",
            "         7.92797953e-02,  6.04599118e-02],\n",
            "       [-7.83001482e-02,  1.68798894e-01,  2.61027694e-01,\n",
            "        -1.17394492e-01, -1.13812238e-01],\n",
            "       [-5.81604898e-01, -8.72868001e-02,  5.12219727e-01,\n",
            "         2.28117332e-01, -6.30621836e-02],\n",
            "       [-1.95325673e-01,  1.17701814e-01,  6.07129991e-01,\n",
            "        -1.04544073e-01, -4.75132883e-01],\n",
            "       [ 2.19776988e-01,  2.65178710e-01,  1.40754238e-01,\n",
            "        -2.78827220e-01, -1.69953644e-01],\n",
            "       [ 2.13059455e-01,  2.59260416e-01,  1.40896425e-01,\n",
            "        -2.72425741e-01, -1.67875364e-01],\n",
            "       [-5.52082598e-01, -1.00836813e-01,  4.45466757e-01,\n",
            "         2.29932919e-01, -6.75447809e-04],\n",
            "       [ 2.14793697e-01,  2.64461756e-01,  1.35594741e-01,\n",
            "        -2.75538951e-01, -1.70973867e-01],\n",
            "       [-4.29500669e-01, -1.38933852e-01,  4.35479492e-01,\n",
            "         2.10100904e-01,  9.70722642e-03],\n",
            "       [ 2.10718989e-01,  2.66877085e-01,  1.38648048e-01,\n",
            "        -2.79471427e-01, -1.68097064e-01],\n",
            "       [ 2.27628097e-01,  2.65107095e-01,  1.32422641e-01,\n",
            "        -2.76434451e-01, -1.72262743e-01],\n",
            "       [-4.94985849e-01, -1.36421010e-01,  4.31112617e-01,\n",
            "         2.50888944e-01,  5.38539700e-02],\n",
            "       [-6.52779359e-03,  1.66868418e-01,  2.37435922e-01,\n",
            "        -1.64166465e-01, -9.02409777e-02],\n",
            "       [ 1.69861376e-01,  2.39053950e-01,  1.61050335e-01,\n",
            "        -2.43647560e-01, -1.54808730e-01],\n",
            "       [-1.01045378e-01,  8.34390447e-02,  2.13062808e-01,\n",
            "        -9.35763642e-02, -1.83447227e-02]], dtype=float32),)\n",
            "[[ 0.5315337   1.0476683   0.57167995 -0.39557034 -1.5171793 ]]\n",
            "['elo', '##n', 'mus', '##k', \"'\", 's', \"'\", 'blast', '##ar', \"'\", 'would', 'be', 'a', 'perfect', 'addition', 'to', 'tesla', 'easter', 'eggs']\n",
            "[101, 21834, 10115, 23139, 10167, 112, 161, 112, 47732, 10370, 112, 11008, 10346, 143, 23021, 15000, 10114, 51571, 58776, 48540, 102]\n",
            "(array([[-1.48236   , -0.895975  ,  0.41832617,  1.1279131 ,  0.57270813]],\n",
            "      dtype=float32),)\n",
            "[[-1.48236    -0.895975    0.41832617  1.1279131   0.57270813]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL49QLltqP-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = encoded_sentences.apply(lambda x : model.predict([x])[0])\n",
        "\n"
      ],
      "execution_count": 338,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHEXNTYyqlEn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "41efecb2-1faf-4408-d21f-b83e4babf06c"
      },
      "source": [
        "print (predictions)\n",
        "print (predictions[0])\n",
        "print (predictions[1][0])\n",
        "\n",
        "rows_list = []\n",
        "for i in range(len(predictions)): \n",
        "\n",
        "        rows_list.append(predictions[i][0])\n",
        "\n",
        "predictions_2 = pd.DataFrame(rows_list) \n",
        "predictions_2.set_index(savedsentences.index, inplace=True)\n",
        "print (predictions_2)\n"
      ],
      "execution_count": 386,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       [[1.313773, 0.8246941, 0.26150295, -0.6092757,...\n",
            "1       [[0.7161402, 0.46309766, 0.0077715274, -0.3159...\n",
            "2       [[0.38432348, 1.4005024, 1.28258, -0.50227904,...\n",
            "3       [[1.3809869, 0.67562056, 0.26868823, -0.566656...\n",
            "4       [[-0.6009043, -0.6274231, -0.038981948, 0.4342...\n",
            "                              ...                        \n",
            "3181    [[0.34391862, -0.21345504, -0.26969814, -0.034...\n",
            "3182    [[-0.728304, -0.55914015, 0.06366488, 0.581215...\n",
            "3183    [[0.5631041, 0.35083732, 0.040439818, -0.14236...\n",
            "3184    [[0.32203043, 0.3292761, 0.3413319, -0.1066241...\n",
            "3185    [[-2.5682073, -1.7864947, 0.6368742, 1.6841565...\n",
            "Length: 3186, dtype: object\n",
            "[[ 1.313773    0.8246941   0.26150295 -0.6092757  -1.4050335 ]]\n",
            "[ 0.7161402   0.46309766  0.00777153 -0.31597665 -0.6624818 ]\n",
            "                   0         1         2         3         4\n",
            "2019-01-03  1.313773  0.824694  0.261503 -0.609276 -1.405033\n",
            "2019-01-03  0.716140  0.463098  0.007772 -0.315977 -0.662482\n",
            "2019-01-03  0.384323  1.400502  1.282580 -0.502279 -2.080056\n",
            "2019-01-03  1.380987  0.675621  0.268688 -0.566657 -1.325519\n",
            "2019-01-03 -0.600904 -0.627423 -0.038982  0.434237  0.547410\n",
            "...              ...       ...       ...       ...       ...\n",
            "2020-03-19  0.343919 -0.213455 -0.269698 -0.034302 -0.021323\n",
            "2020-03-19 -0.728304 -0.559140  0.063665  0.581215  0.456070\n",
            "2020-03-19  0.563104  0.350837  0.040440 -0.142362 -0.704715\n",
            "2020-03-19  0.322030  0.329276  0.341332 -0.106624 -0.793826\n",
            "2020-03-19 -2.568207 -1.786495  0.636874  1.684157  1.506772\n",
            "\n",
            "[3186 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKUHsrxCSVAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# predictions_2 = predictions.copy().to_frame()\n",
        "# predictions_2.set_index(savedsentences.index, inplace=True)\n",
        "# print (predictions_2)\n"
      ],
      "execution_count": 387,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhsKGHxButHf",
        "colab_type": "text"
      },
      "source": [
        "# Here we should get the average stars for each transaction day."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VxnQbSXSaee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_predictions = predictions_2.groupby(predictions_2.index).apply(np.mean)\n",
        "avg_predictions_array = np.array([a for a in avg_predictions.values])\n",
        "shape = avg_predictions_array.shape\n",
        "pd.to_pickle(avg_predictions, \"sentiment_predictions_tesla_with_tokens\")"
      ],
      "execution_count": 390,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnNMGmJ_Sd2m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "108c9e23-5bf1-4e43-eb53-8022d4ca73e5"
      },
      "source": [
        "avg_predictions.shape\n",
        "\n",
        "\n",
        "avg_predictions.columns = ['1star', '2star','3star', '4star','5star']\n",
        "\n",
        "print (avg_predictions)"
      ],
      "execution_count": 416,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               1star     2star     3star     4star     5star\n",
            "2019-01-03  0.275092  0.341758  0.343074 -0.155093 -0.685328\n",
            "2019-01-04  0.441897  0.313300  0.170157 -0.129865 -0.645471\n",
            "2019-01-05  0.341846  0.469301  0.497733 -0.122689 -0.968765\n",
            "2019-01-06 -0.336655 -0.333981  0.117318  0.392819  0.007872\n",
            "2019-01-07 -0.260595  0.063817  0.328041  0.234532 -0.391939\n",
            "...              ...       ...       ...       ...       ...\n",
            "2020-03-15  0.806598  0.464286  0.072859 -0.320151 -0.835587\n",
            "2020-03-16  0.532214  0.118662  0.047815 -0.141394 -0.477883\n",
            "2020-03-17  0.730104  0.302730  0.089542 -0.255150 -0.715610\n",
            "2020-03-18  0.140813  0.144669  0.169886  0.063744 -0.463030\n",
            "2020-03-19 -0.334883 -0.355477  0.111074  0.395313  0.083074\n",
            "\n",
            "[434 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znzIqbmZAGwf",
        "colab_type": "text"
      },
      "source": [
        "# Here I should link the tesla stock data, and its associated date. Using the above features to do the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPJfvD7ru9EV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "20899bc8-327a-469d-8005-7aaf3c3e65aa"
      },
      "source": [
        "import yfinance as yf\n",
        "tickers = ['TSLA']\n",
        "ohlc = yf.download(tickers, start=\"2019-01-03\", end=\"2020-03-20\")\n",
        "print(list(ohlc.columns.values))\n",
        "#ohlc.head()\n",
        "ohlc_reversed = ohlc.sort_values(['Date'], ascending=[True])\n",
        "ohlc_reversed = ohlc_reversed.reindex(columns=['Open', 'High', 'Low', 'Volume','Adj Close', 'Close'])\n",
        "ohlc_reversed.drop('Adj Close', axis=1, inplace=True)\n",
        "print (ohlc_reversed)"
      ],
      "execution_count": 414,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
            "                  Open        High         Low    Volume       Close\n",
            "Date                                                                \n",
            "2019-01-03  307.000000  309.399994  297.380005   6965200  300.359985\n",
            "2019-01-04  306.000000  318.000000  302.730011   7394100  317.690002\n",
            "2019-01-07  321.720001  336.739990  317.750000   7551200  334.959991\n",
            "2019-01-08  341.959991  344.010010  327.019989   7008500  335.350006\n",
            "2019-01-09  335.500000  343.500000  331.470001   5432900  338.529999\n",
            "...                ...         ...         ...       ...         ...\n",
            "2020-03-13  595.000000  607.570007  502.000000  22640300  546.619995\n",
            "2020-03-16  469.500000  494.869995  442.170013  20489500  445.070007\n",
            "2020-03-17  440.010010  471.850006  396.000000  23994600  430.200012\n",
            "2020-03-18  389.000000  404.859985  350.510010  23786200  361.220001\n",
            "2020-03-19  374.700012  452.000000  358.459991  30195500  427.640015\n",
            "\n",
            "[305 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKXdElDkB0gU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "fc19bc52-f318-494b-bd66-d7a292ff2421"
      },
      "source": [
        "df = ohlc_reversed.merge(avg_predictions, left_index=True, right_index=True)\n",
        "df = df.reindex(columns=['1star' ,'2star' ,'3star', '4star','5star' ,'Open', 'High', 'Low', 'Volume', 'Close'])\n",
        "print (df)\n",
        "df.head()"
      ],
      "execution_count": 419,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               1star     2star     3star  ...         Low    Volume       Close\n",
            "2019-01-03  0.275092  0.341758  0.343074  ...  297.380005   6965200  300.359985\n",
            "2019-01-04  0.441897  0.313300  0.170157  ...  302.730011   7394100  317.690002\n",
            "2019-01-07 -0.260595  0.063817  0.328041  ...  317.750000   7551200  334.959991\n",
            "2019-01-08 -0.651154 -0.505755  0.085503  ...  327.019989   7008500  335.350006\n",
            "2019-01-09 -0.661772 -0.323006  0.131057  ...  331.470001   5432900  338.529999\n",
            "...              ...       ...       ...  ...         ...       ...         ...\n",
            "2020-03-12  0.380618  0.402513  0.369910  ...  546.250000  18909100  560.549988\n",
            "2020-03-16  0.532214  0.118662  0.047815  ...  442.170013  20489500  445.070007\n",
            "2020-03-17  0.730104  0.302730  0.089542  ...  396.000000  23994600  430.200012\n",
            "2020-03-18  0.140813  0.144669  0.169886  ...  350.510010  23786200  361.220001\n",
            "2020-03-19 -0.334883 -0.355477  0.111074  ...  358.459991  30195500  427.640015\n",
            "\n",
            "[302 rows x 10 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1star</th>\n",
              "      <th>2star</th>\n",
              "      <th>3star</th>\n",
              "      <th>4star</th>\n",
              "      <th>5star</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Close</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2019-01-03</th>\n",
              "      <td>0.275092</td>\n",
              "      <td>0.341758</td>\n",
              "      <td>0.343074</td>\n",
              "      <td>-0.155093</td>\n",
              "      <td>-0.685328</td>\n",
              "      <td>307.000000</td>\n",
              "      <td>309.399994</td>\n",
              "      <td>297.380005</td>\n",
              "      <td>6965200</td>\n",
              "      <td>300.359985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-01-04</th>\n",
              "      <td>0.441897</td>\n",
              "      <td>0.313300</td>\n",
              "      <td>0.170157</td>\n",
              "      <td>-0.129865</td>\n",
              "      <td>-0.645471</td>\n",
              "      <td>306.000000</td>\n",
              "      <td>318.000000</td>\n",
              "      <td>302.730011</td>\n",
              "      <td>7394100</td>\n",
              "      <td>317.690002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-01-07</th>\n",
              "      <td>-0.260595</td>\n",
              "      <td>0.063817</td>\n",
              "      <td>0.328041</td>\n",
              "      <td>0.234532</td>\n",
              "      <td>-0.391939</td>\n",
              "      <td>321.720001</td>\n",
              "      <td>336.739990</td>\n",
              "      <td>317.750000</td>\n",
              "      <td>7551200</td>\n",
              "      <td>334.959991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-01-08</th>\n",
              "      <td>-0.651154</td>\n",
              "      <td>-0.505755</td>\n",
              "      <td>0.085503</td>\n",
              "      <td>0.549835</td>\n",
              "      <td>0.319000</td>\n",
              "      <td>341.959991</td>\n",
              "      <td>344.010010</td>\n",
              "      <td>327.019989</td>\n",
              "      <td>7008500</td>\n",
              "      <td>335.350006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-01-09</th>\n",
              "      <td>-0.661772</td>\n",
              "      <td>-0.323006</td>\n",
              "      <td>0.131057</td>\n",
              "      <td>0.541672</td>\n",
              "      <td>0.091894</td>\n",
              "      <td>335.500000</td>\n",
              "      <td>343.500000</td>\n",
              "      <td>331.470001</td>\n",
              "      <td>5432900</td>\n",
              "      <td>338.529999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               1star     2star     3star  ...         Low   Volume       Close\n",
              "2019-01-03  0.275092  0.341758  0.343074  ...  297.380005  6965200  300.359985\n",
              "2019-01-04  0.441897  0.313300  0.170157  ...  302.730011  7394100  317.690002\n",
              "2019-01-07 -0.260595  0.063817  0.328041  ...  317.750000  7551200  334.959991\n",
              "2019-01-08 -0.651154 -0.505755  0.085503  ...  327.019989  7008500  335.350006\n",
              "2019-01-09 -0.661772 -0.323006  0.131057  ...  331.470001  5432900  338.529999\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 419
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJmqFaKhHNNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #### Only use stock data, no news features.\n",
        "\n",
        "# df = ohlc_reversed\n",
        "\n",
        "# print (df)"
      ],
      "execution_count": 435,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sawpGuZgCMLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def standard_scaler(X_train, X_test):\n",
        "    train_samples, train_nx, train_ny = X_train.shape\n",
        "    test_samples, test_nx, test_ny = X_test.shape\n",
        "    \n",
        "    X_train = X_train.reshape((train_samples, train_nx * train_ny))\n",
        "    X_test = X_test.reshape((test_samples, test_nx * test_ny))\n",
        "    \n",
        "    preprocessor = prep.StandardScaler().fit(X_train)\n",
        "    X_train = preprocessor.transform(X_train)\n",
        "    X_test = preprocessor.transform(X_test)\n",
        "    \n",
        "    X_train = X_train.reshape((train_samples, train_nx, train_ny))\n",
        "    X_test = X_test.reshape((test_samples, test_nx, test_ny))\n",
        "    \n",
        "    return X_train, X_test\n",
        "\n",
        "def preprocess_data(stock, seq_len):\n",
        "    amount_of_features = len(stock.columns)\n",
        "    data = stock.values\n",
        "    \n",
        "    sequence_length = seq_len + 1\n",
        "    result = []\n",
        "    ## Probably here add another column as the label.\n",
        "    for index in range(len(data) - sequence_length):\n",
        "        result.append(data[index : index + sequence_length])\n",
        "        \n",
        "    result = np.array(result)\n",
        "    row = round(0.9 * result.shape[0])\n",
        "    train = result[: int(row), :]\n",
        "\n",
        "    # print (result)\n",
        "    # print (row)\n",
        "    print (train[:, : -1])\n",
        "    print (train[:, -1][: ,-1]) \n",
        "\n",
        "    print (result[int(row) :, -1][ : ,-1]) \n",
        "\n",
        "    train, result = standard_scaler(train, result)\n",
        "    \n",
        "    X_train = train[:, : -1]\n",
        "    y_train = train[:, -1][: ,-1]\n",
        "    X_test = result[int(row) :, : -1]\n",
        "    y_test = result[int(row) :, -1][ : ,-1]\n",
        "\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], amount_of_features))\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], amount_of_features)) \n",
        "\n",
        " \n",
        "\n",
        "    return [X_train, y_train, X_test, y_test]\n",
        "\n",
        "def build_model(layers):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(512, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(LSTM(256, return_sequences=True))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(LSTM(128, return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(64, return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(32))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    start = time.time()\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "    model.summary() \n",
        "    print(\"Compilation Time : \", time.time() - start)\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 436,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgsiEFVjCYq_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "08d71e89-986c-47f6-b0ec-2df72a415666"
      },
      "source": [
        "import time\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.layers.recurrent import LSTM\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.preprocessing as prep\n",
        "\n",
        "window = 15\n",
        "X_train, y_train, X_test, y_test = preprocess_data(df[:: -1], window)\n",
        "print(\"X_train\", X_train.shape)\n",
        "print(\"y_train\", y_train.shape)\n",
        "print(\"X_test\", X_test.shape)\n",
        "print(\"y_test\", y_test.shape)\n",
        "# print (X_train)\n",
        "# print (y_train)\n",
        "# print (X_test)\n",
        "# print (y_test)\n",
        "\n",
        "model = build_model([X_train.shape[2], window, 100, 1])"
      ],
      "execution_count": 437,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[3.74700012e+02 4.52000000e+02 3.58459991e+02 3.01955000e+07\n",
            "   4.27640015e+02]\n",
            "  [3.89000000e+02 4.04859985e+02 3.50510010e+02 2.37862000e+07\n",
            "   3.61220001e+02]\n",
            "  [4.40010010e+02 4.71850006e+02 3.96000000e+02 2.39946000e+07\n",
            "   4.30200012e+02]\n",
            "  ...\n",
            "  [8.05000000e+02 8.06979980e+02 7.16109985e+02 2.57840000e+07\n",
            "   7.45510010e+02]\n",
            "  [7.11260010e+02 7.43690002e+02 6.86669983e+02 2.01950000e+07\n",
            "   7.43619995e+02]\n",
            "  [6.29700012e+02 6.90520020e+02 6.11520020e+02 2.45642000e+07\n",
            "   6.67989990e+02]]\n",
            "\n",
            " [[3.89000000e+02 4.04859985e+02 3.50510010e+02 2.37862000e+07\n",
            "   3.61220001e+02]\n",
            "  [4.40010010e+02 4.71850006e+02 3.96000000e+02 2.39946000e+07\n",
            "   4.30200012e+02]\n",
            "  [4.69500000e+02 4.94869995e+02 4.42170013e+02 2.04895000e+07\n",
            "   4.45070007e+02]\n",
            "  ...\n",
            "  [7.11260010e+02 7.43690002e+02 6.86669983e+02 2.01950000e+07\n",
            "   7.43619995e+02]\n",
            "  [6.29700012e+02 6.90520020e+02 6.11520020e+02 2.45642000e+07\n",
            "   6.67989990e+02]\n",
            "  [7.30000000e+02 7.39770020e+02 6.69000000e+02 2.41493000e+07\n",
            "   6.79000000e+02]]\n",
            "\n",
            " [[4.40010010e+02 4.71850006e+02 3.96000000e+02 2.39946000e+07\n",
            "   4.30200012e+02]\n",
            "  [4.69500000e+02 4.94869995e+02 4.42170013e+02 2.04895000e+07\n",
            "   4.45070007e+02]\n",
            "  [5.95000000e+02 6.07570007e+02 5.02000000e+02 2.26403000e+07\n",
            "   5.46619995e+02]\n",
            "  ...\n",
            "  [6.29700012e+02 6.90520020e+02 6.11520020e+02 2.45642000e+07\n",
            "   6.67989990e+02]\n",
            "  [7.30000000e+02 7.39770020e+02 6.69000000e+02 2.41493000e+07\n",
            "   6.79000000e+02]\n",
            "  [7.82500000e+02 8.13309998e+02 7.76109985e+02 1.40855000e+07\n",
            "   7.78799988e+02]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[2.83899994e+02 2.91989990e+02 2.82700012e+02 6.84470000e+06\n",
            "   2.88959991e+02]\n",
            "  [2.86489990e+02 2.88070007e+02 2.81059998e+02 7.50410000e+06\n",
            "   2.83359985e+02]\n",
            "  [2.83519989e+02 2.91279999e+02 2.80500000e+02 7.39230000e+06\n",
            "   2.90920013e+02]\n",
            "  ...\n",
            "  [2.97910004e+02 3.02899994e+02 2.97000000e+02 6.62650000e+06\n",
            "   2.98769989e+02]\n",
            "  [2.94489990e+02 2.96500000e+02 2.92100006e+02 5.74060000e+06\n",
            "   2.94709991e+02]\n",
            "  [3.01809998e+02 3.03239990e+02 2.90500000e+02 8.90920000e+06\n",
            "   2.91230011e+02]]\n",
            "\n",
            " [[2.86489990e+02 2.88070007e+02 2.81059998e+02 7.50410000e+06\n",
            "   2.83359985e+02]\n",
            "  [2.83519989e+02 2.91279999e+02 2.80500000e+02 7.39230000e+06\n",
            "   2.90920013e+02]\n",
            "  [2.76910004e+02 2.85589996e+02 2.75890015e+02 8.81960000e+06\n",
            "   2.84140015e+02]\n",
            "  ...\n",
            "  [2.94489990e+02 2.96500000e+02 2.92100006e+02 5.74060000e+06\n",
            "   2.94709991e+02]\n",
            "  [3.01809998e+02 3.03239990e+02 2.90500000e+02 8.90920000e+06\n",
            "   2.91230011e+02]\n",
            "  [3.04410004e+02 3.06299988e+02 2.99000000e+02 7.14210000e+06\n",
            "   3.02559998e+02]]\n",
            "\n",
            " [[2.83519989e+02 2.91279999e+02 2.80500000e+02 7.39230000e+06\n",
            "   2.90920013e+02]\n",
            "  [2.76910004e+02 2.85589996e+02 2.75890015e+02 8.81960000e+06\n",
            "   2.84140015e+02]\n",
            "  [2.78839996e+02 2.84700012e+02 2.74250000e+02 9.44250000e+06\n",
            "   2.76589996e+02]\n",
            "  ...\n",
            "  [3.01809998e+02 3.03239990e+02 2.90500000e+02 8.90920000e+06\n",
            "   2.91230011e+02]\n",
            "  [3.04410004e+02 3.06299988e+02 2.99000000e+02 7.14210000e+06\n",
            "   3.02559998e+02]\n",
            "  [3.06559998e+02 3.11540009e+02 3.05470001e+02 4.16840000e+06\n",
            "   3.05640015e+02]]]\n",
            "[679.         778.79998779 799.90997314 833.78997803 901.\n",
            " 899.40997314 917.41998291 858.40002441 800.0300293  804.\n",
            " 767.28997803 774.38000488 771.2800293  748.07000732 748.96002197\n",
            " 734.70001221 887.05999756 780.         650.57000732 640.80999756\n",
            " 580.98999023 566.90002441 558.02001953 564.82000732 572.20001221\n",
            " 569.55999756 547.20001221 510.5        513.48999023 518.5\n",
            " 537.91998291 524.85998535 478.1499939  481.33999634 492.14001465\n",
            " 469.05999756 451.54000854 443.01000977 430.26000977 418.32998657\n",
            " 414.70001221 430.38000488 430.94000244 425.25       419.22000122\n",
            " 405.58999634 404.04000854 393.1499939  378.98999023 381.5\n",
            " 358.39001465 359.67999268 352.70001221 348.83999634 339.52999878\n",
            " 335.89001465 330.36999512 333.02999878 336.20001221 334.86999512\n",
            " 329.94000244 331.29000854 328.92001343 336.33999634 333.04000854\n",
            " 354.82998657 352.22000122 359.51998901 349.98999023 352.17001343\n",
            " 349.3500061  346.10998535 349.92999268 345.08999634 337.14001465\n",
            " 335.54000854 326.57998657 317.22000122 317.47000122 313.30999756\n",
            " 314.92001343 315.01000977 316.22000122 327.70999146 328.13000488\n",
            " 299.67999268 254.67999268 255.58000183 253.5        256.95001221\n",
            " 261.97000122 259.75       257.89001465 256.95999146 247.88999939\n",
            " 244.74000549 244.52999878 240.05000305 237.72000122 231.42999268\n",
            " 233.02999878 243.13000488 244.69000244 240.86999512 242.13000488\n",
            " 242.55999756 228.69999695 223.21000671 241.22999573 240.61999512\n",
            " 246.6000061  243.49000549 244.78999329 242.80999756 245.19999695\n",
            " 245.86999512 247.1000061  235.53999329 231.78999329 227.44999695\n",
            " 229.58000183 220.67999268 225.00999451 225.61000061 221.71000671\n",
            " 215.58999634 214.08000183 215.         211.3999939  222.1499939\n",
            " 220.83000183 225.86000061 226.83000183 219.94000244 215.63999939\n",
            " 219.61999512 235.         229.00999451 235.00999451 238.30000305\n",
            " 233.41999817 230.75       228.32000732 234.33999634 233.8500061\n",
            " 241.61000061 242.25999451 235.77000427 228.03999329 228.82000732\n",
            " 264.88000488 260.17001343 255.67999268 258.17999268 253.53999329\n",
            " 254.86000061 252.38000488 253.5        245.08000183 238.6000061\n",
            " 238.91999817 230.05999756 230.33999634 233.1000061  234.8999939\n",
            " 224.55000305 227.16999817 223.46000671 222.83999634 219.27000427\n",
            " 219.75999451 223.63999939 221.86000061 219.61999512 226.42999268\n",
            " 224.74000549 225.02999878 214.91999817 213.91000366 209.25999451\n",
            " 217.1000061  212.88000488 204.5        205.94999695 196.58999634\n",
            " 193.6000061  178.97000122 185.16000366 188.22000122 189.86000061\n",
            " 188.69999695 190.63000488 195.49000549 192.72999573 205.08000183\n",
            " 205.36000061 211.02999878 228.33000183 231.94999695 232.30999756\n",
            " 227.00999451 239.52000427 241.97999573 244.83999634 247.05999756\n",
            " 255.33999634 255.02999878 244.1000061  234.00999451 238.69000244\n",
            " 241.47000122 235.13999939 247.63000488 258.66000366 263.8999939\n",
            " 262.75       273.26000977 271.23001099 273.35998535 266.38000488\n",
            " 267.70001221 268.42001343 276.05999756 272.30999756 273.20001221\n",
            " 274.95999146 267.77999878 291.80999756 285.88000488 289.17999268\n",
            " 279.85998535 278.61999512 274.82998657 267.76998901 260.42001343\n",
            " 264.52999878 274.01998901 273.6000061  267.47000122 269.48999023\n",
            " 275.42999268 289.95999146 288.95999146 283.35998535 290.92001343\n",
            " 284.14001465 276.58999634 276.23999023 276.54000854 285.35998535\n",
            " 294.79000854 319.88000488 314.73999023 297.85998535 298.76998901\n",
            " 294.70999146 291.23001099 302.55999756 305.64001465 307.88000488]\n",
            "[303.76998901 308.17001343 311.80999756 312.83999634 305.79998779\n",
            " 307.51000977 317.22000122 321.3500061  312.89001465 312.20999146\n",
            " 307.01998901 308.76998901 297.45999146 296.38000488 297.04000854\n",
            " 291.51000977 287.58999634 298.92001343 302.26000977 347.30999756\n",
            " 346.04998779 344.42999268 334.3999939  347.26000977 344.97000122\n",
            " 338.52999878 335.3500061  334.95999146 317.69000244]\n",
            "X_train (260, 15, 5)\n",
            "y_train (260,)\n",
            "X_test (29, 15, 5)\n",
            "y_test (29,)\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_20 (LSTM)               (None, 15, 512)           1060864   \n",
            "_________________________________________________________________\n",
            "dropout_741 (Dropout)        (None, 15, 512)           0         \n",
            "_________________________________________________________________\n",
            "lstm_21 (LSTM)               (None, 15, 256)           787456    \n",
            "_________________________________________________________________\n",
            "dropout_742 (Dropout)        (None, 15, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_22 (LSTM)               (None, 15, 128)           197120    \n",
            "_________________________________________________________________\n",
            "dropout_743 (Dropout)        (None, 15, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm_23 (LSTM)               (None, 15, 64)            49408     \n",
            "_________________________________________________________________\n",
            "dropout_744 (Dropout)        (None, 15, 64)            0         \n",
            "_________________________________________________________________\n",
            "lstm_24 (LSTM)               (None, 32)                12416     \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 2,107,297\n",
            "Trainable params: 2,107,297\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Compilation Time :  0.012554168701171875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKp__D3BCxEW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f2289b89-7ee0-4997-e8b8-47d683ee5e0b"
      },
      "source": [
        "model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=768,\n",
        "    epochs=300,\n",
        "    validation_split=0.1,\n",
        "    verbose=0)\n",
        "\n",
        "trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
        "print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
        "\n",
        "testScore = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))"
      ],
      "execution_count": 438,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Score: 0.01 MSE (0.09 RMSE)\n",
            "Test Score: 0.03 MSE (0.18 RMSE)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrpIOGWzC4uJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "e64b9119-527c-4471-e446-b4c19767cbfc"
      },
      "source": [
        "diff = []\n",
        "ratio = []\n",
        "pred = model.predict(X_test)\n",
        "for u in range(len(y_test)):\n",
        "    pr = pred[u][0]\n",
        "    ratio.append((y_test[u] / pr) - 1)\n",
        "    diff.append(abs(y_test[u] - pr))\n",
        "\n",
        "print (pred)\n",
        "print (y_test)"
      ],
      "execution_count": 439,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.2216009 ]\n",
            " [-0.22807989]\n",
            " [-0.23624554]\n",
            " [-0.24170636]\n",
            " [-0.23946685]\n",
            " [-0.20891069]\n",
            " [-0.12349685]\n",
            " [-0.09461281]\n",
            " [-0.05415035]\n",
            " [-0.02681252]\n",
            " [-0.01875914]\n",
            " [-0.02064643]\n",
            " [-0.00181328]\n",
            " [-0.01809187]\n",
            " [-0.07716522]\n",
            " [-0.13616778]\n",
            " [-0.17900605]\n",
            " [-0.21752053]\n",
            " [-0.24661738]\n",
            " [-0.26122993]\n",
            " [-0.27153358]\n",
            " [-0.26452273]\n",
            " [-0.24286568]\n",
            " [-0.19447026]\n",
            " [-0.13387457]\n",
            " [-0.07713941]\n",
            " [-0.0178118 ]\n",
            " [ 0.01664967]\n",
            " [ 0.00242738]]\n",
            "[-0.13926678 -0.11147547 -0.08848471 -0.08197906 -0.12644496 -0.11564417\n",
            " -0.0543142  -0.02822839 -0.08166314 -0.08595828 -0.11873922 -0.10768593\n",
            " -0.1791218  -0.18594318 -0.18177449 -0.2167029  -0.24146238 -0.16990005\n",
            " -0.14880407  0.13573936  0.12778092  0.11754875  0.05419757  0.13542362\n",
            "  0.12095954  0.08028338  0.06019801  0.05773461 -0.0513456 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBNNtkN7C8Sz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "ac7958ca-05c4-441a-bb99-a038df69feed"
      },
      "source": [
        "%matplotlib inline \n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt2\n",
        "\n",
        "plt2.plot(pred, color='red', label='Prediction')\n",
        "plt2.plot(y_test, color='blue', label='Ground Truth')\n",
        "plt2.legend(loc='upper left')\n",
        "plt2.show()"
      ],
      "execution_count": 440,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}